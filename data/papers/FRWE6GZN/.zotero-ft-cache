LITERA Reader
25
864
Algorithm Design
PAGE VIEW (BETA VERSION)
0
/
0
Match case
Limit results 1 per page
Preface 


Algorithmic ideas are pervasive, and their reach is apparent in examples both 


within computer science and beyond. Some of the major shifts in Internet 


routing standards can be viewed as debates over the deﬁciencies of one 


shortest-path algorithm and the relative advantages of another. The basic 


notions used by biologists to express similarities among genes and genomes 


have algorithmic deﬁnitions. The concerns voiced by economists over the 


feasibility of combinatorial auctions in practice are rooted partly in the fact that 


these auctions contain computationally intractable search problems as special 


cases. And algorithmic notions aren’t just restricted to well-known and long- 


standing problems; one sees the reﬂections of these ideas on a regular basis, 


in novel issues arising across a wide range of areas. The scientist from Yahoo! 


who told us over lunch one day about their system for serving ads to users was 


describing a set of issues that, deep down, could be modeled as a network ﬂow 


problem. So was the former student, now a management consultant working 


on stafﬁng protocols for large hospitals, whom we happened to meet on a trip 


to New York City. 


The point is not simply that algorithms have many applications. The 


deeper issue is that the subject of algorithms is a powerful lens through which 


to view the ﬁeld of computer science in general. Algorithmic problems form 


the heart of computer science, but they rarely arrive as cleanly packaged, 


mathematically precise questions. Rather, they tend to come bundled together 


with lots of messy, application-speciﬁc detail, some of it essential, some of it 


extraneous. As a result, the algorithmic enterprise consists of two fundamental 


components: the task of getting to the mathematically clean core of a problem, 


and then the task of identifying the appropriate algorithm design techniques, 


based on the structure of the problem. These two components interact: the 


more comfortable one is with the full array of possible design techniques, 


the more one starts to recognize the clean formulations that lie within messy 
xiv 


Preface 


problems out in the world. At their most effective, then, algorithmic ideas do 


not just provide solutions to well-posed problems; they form the language that 


lets you cleanly express the underlying questions. 


The goal of our book is to convey this approach to algorithms, as a design 


process that begins with problems arising across the full range of computing 


applications, builds on an understanding of algorithm design techniques, and 


results in the development of efﬁcient solutions to these problems. We seek 


to explore the role of algorithmic ideas in computer science generally, and 


relate these ideas to the range of precisely formulated problems for which we 


can design and analyze algorithms. In other words, what are the underlying 


issues that motivate these problems, and how did we choose these particular 


ways of formulating them? How did we recognize which design principles were 


appropriate in different situations? 


In keeping with this, our goal is to offer advice on how to identify clean 


algorithmic problem formulations in complex issues from different areas of 


computing and, from this, how to design efﬁcient algorithms for the resulting 


problems. Sophisticated algorithms are often best understood by reconstruct- 


ing the sequence of ideas—including false starts and dead ends—that led from 


simpler initial approaches to the eventual solution. The result is a style of ex- 


position that does not take the most direct route from problem statement to 


algorithm, but we feel it better reﬂects the way that we and our colleagues 


genuinely think about these questions. 


Overview 


The book is intended for students who have completed a programming- 


based two-semester introductory computer science sequence (the standard 


“CS1/CS2” courses) in which they have written programs that implement 


basic algorithms, manipulate discrete structures such as trees and graphs, and 


apply basic data structures such as arrays, lists, queues, and stacks. Since 


the interface between CS1/CS2 and a ﬁrst algorithms course is not entirely 


standard, we begin the book with self-contained coverage of topics that at 


some institutions are familiar to students from CS1/CS2, but which at other 


institutions are included in the syllabi of the ﬁrst algorithms course. This 


material can thus be treated either as a review or as new material; by including 


it, we hope the book can be used in a broader array of courses, and with more 


ﬂexibility in the prerequisite knowledge that is assumed. 


In keeping with the approach outlined above, we develop the basic algo- 


rithm design techniques by drawing on problems from across many areas of 


computer science and related ﬁelds. To mention a few representative examples 


here, we include fairly detailed discussions of applications from systems and 


networks (caching, switching, interdomain routing on the Internet), artiﬁcial 
Preface 


xv 


intelligence (planning, game playing, Hopﬁeld networks), computer vision 


(image segmentation), data mining (change-point detection, clustering), op- 


erations research (airline scheduling), and computational biology (sequence 


alignment, RNA secondary structure). 


The notion of computational intractability, and NP-completeness in par- 


ticular, plays a large role in the book. This is consistent with how we think 


about the overall process of algorithm design. Some of the time, an interest- 


ing problem arising in an application area will be amenable to an efﬁcient 


solution, and some of the time it will be provably NP-complete; in order to 


fully address a new algorithmic problem, one should be able to explore both 


of these options with equal familiarity. Since so many natural problems in 


computer science are NP-complete, the development of methods to deal with 


intractable problems has become a crucial issue in the study of algorithms, 


and our book heavily reﬂects this theme. The discovery that a problem is NP- 


complete should not be taken as the end of the story, but as an invitation to 


begin looking for approximation algorithms, heuristic local search techniques, 


or tractable special cases. We include extensive coverage of each of these three 


approaches. 


Problems and Solved Exercises 


An important feature of the book is the collection of problems. Across all 


chapters, the book includes over 200 problems, almost all of them developed 


and class-tested in homework or exams as part of our teaching of the course 


at Cornell. We view the problems as a crucial component of the book, and 


they are structured in keeping with our overall approach to the material. Most 


of them consist of extended verbal descriptions of a problem arising in an 


application area in computer science or elsewhere out in the world, and part of 


the problem is to practice what we discuss in the text: setting up the necessary 


notation and formalization, designing an algorithm, and then analyzing it and 


proving it correct. (We view a complete answer to one of these problems as 


consisting of all these components: a fully explained algorithm, an analysis of 


the running time, and a proof of correctness.) The ideas for these problems 


come in large part from discussions we have had over the years with people 


working in different areas, and in some cases they serve the dual purpose of 


recording an interesting (though manageable) application of algorithms that 


we haven’t seen written down anywhere else. 


To help with the process of working on these problems, we include in 


each chapter a section entitled “Solved Exercises,” where we take one or more 


problems and describe how to go about formulating a solution. The discussion 


devoted to each solved exercise is therefore signiﬁcantly longer than what 


would be needed simply to write a complete, correct solution (in other words, 
xvi 


Preface 


signiﬁcantly longer than what it would take to receive full credit if these were 


being assigned as homework problems). Rather, as with the rest of the text, 


the discussions in these sections should be viewed as trying to give a sense 


of the larger process by which one might think about problems of this type, 


culminating in the speciﬁcation of a precise solution. 


It is worth mentioning two points concerning the use of these problems 


as homework in a course. First, the problems are sequenced roughly in order 


of increasing difﬁculty, but this is only an approximate guide and we advise 


against placing too much weight on it: since the bulk of the problems were 


designed as homework for our undergraduate class, large subsets of the 


problems in each chapter are really closely comparable in terms of difﬁculty. 


Second, aside from the lowest-numbered ones, the problems are designed to 


involve some investment of time, both to relate the problem description to the 


algorithmic techniques in the chapter, and then to actually design the necessary 


algorithm. In our undergraduate class, we have tended to assign roughly three 


of these problems per week. 


Pedagogical Features and Supplements 


In addition to the problems and solved exercises, the book has a number of 


further pedagogical features, as well as additional supplements to facilitate its 


use for teaching. 


As noted earlier, a large number of the sections in the book are devoted 


to the formulation of an algorithmic problem—including its background and 


underlying motivation—and the design and analysis of an algorithm for this 


problem. To reﬂect this style, these sections are consistently structured around 


a sequence of subsections: “The Problem,” where the problem is described 


and a precise formulation is worked out; “Designing the Algorithm,” where 


the appropriate design technique is employed to develop an algorithm; and 


“Analyzing the Algorithm,” which proves properties of the algorithm and 


analyzes its efﬁciency. These subsections are highlighted in the text with an 


icon depicting a feather. In cases where extensions to the problem or further 


analysis of the algorithm is pursued, there are additional subsections devoted 


to these issues. The goal of this structure is to offer a relatively uniform style 


of presentation that moves from the initial discussion of a problem arising in a 


computing application through to the detailed analysis of a method to solve it. 


A number of supplements are available in support of the book itself. An 


instructor’s manual works through all the problems, providing full solutions to 


each. A set of lecture slides, developed by Kevin Wayne of Princeton University, 


is also available; these slides follow the order of the book’s sections and can 


thus be used as the foundation for lectures in a course based on the book. These 


ﬁles are available at 
www.aw.com
. For instructions on obtaining a professor 
Preface 


xvii 


login and password, search the site for either “Kleinberg” or “Tardos” or 


contact your local Addison-Wesley representative. 


Finally, we would appreciate receiving feedback on the book. In particular, 


as in any book of this length, there are undoubtedly errors that have remained 


in the ﬁnal version. Comments and reports of errors can be sent to us by e-mail, 


at the address [email protected]; please include the word “feedback” 


in the subject line of the message. 


Chapter-by-Chapter Synopsis 


Chapter 1 starts by introducing some representative algorithmic problems. We 


begin immediately with the Stable Matching Problem, since we feel it sets 


up the basic issues in algorithm design more concretely and more elegantly 


than any abstract discussion could: stable matching is motivated by a natural 


though complex real-world issue, from which one can abstract an interesting 


problem statement and a surprisingly effective algorithm to solve this problem. 


The remainder of Chapter 1 discusses a list of ﬁve “representative problems” 


that foreshadow topics from the remainder of the course. These ﬁve problems 


are interrelated in the sense that they are all variations and/or special cases 


of the Independent Set Problem; but one is solvable by a greedy algorithm, 


one by dynamic programming, one by network ﬂow, one (the Independent 


Set Problem itself) is NP-complete, and one is PSPACE-complete. The fact that 


closely related problems can vary greatly in complexity is an important theme 


of the book, and these ﬁve problems serve as milestones that reappear as the 


book progresses. 


Chapters 2 and 3 cover the interface to the CS1/CS2 course sequence 


mentioned earlier. Chapter 2 introduces the key mathematical deﬁnitions and 


notations used for analyzing algorithms, as well as the motivating principles 


behind them. It begins with an informal overview of what it means for a prob- 


lem to be computationally tractable, together with the concept of polynomial 


time as a formal notion of efﬁciency. It then discusses growth rates of func- 


tions and asymptotic analysis more formally, and offers a guide to commonly 


occurring functions in algorithm analysis, together with standard applications 


in which they arise. Chapter 3 covers the basic deﬁnitions and algorithmic 


primitives needed for working with graphs, which are central to so many of 


the problems in the book. A number of basic graph algorithms are often im- 


plemented by students late in the CS1/CS2 course sequence, but it is valuable 


to present the material here in a broader algorithm design context. In par- 


ticular, we discuss basic graph deﬁnitions, graph traversal techniques such 


as breadth-ﬁrst search and depth-ﬁrst search, and directed graph concepts 


including strong connectivity and topological ordering. 
xviii 


Preface 


Chapters 2 and 3 also present many of the basic data structures that will 


be used for implementing algorithms throughout the book; more advanced 


data structures are presented in subsequent chapters. Our approach to data 


structures is to introduce them as they are needed for the implementation of 


the algorithms being developed in the book. Thus, although many of the data 


structures covered here will be familiar to students from the CS1/CS2 sequence, 


our focus is on these data structures in the broader context of algorithm design 


and analysis. 


Chapters 4 through 7 cover four major algorithm design techniques: greedy 


algorithms, divide and conquer, dynamic programming, and network ﬂow. 


With greedy algorithms, the challenge is to recognize when they work and 


when they don’t; our coverage of this topic is centered around a way of clas- 


sifying the kinds of arguments used to prove greedy algorithms correct. This 


chapter concludes with some of the main applications of greedy algorithms, 


for shortest paths, undirected and directed spanning trees, clustering, and 


compression. For divide and conquer, we begin with a discussion of strategies 


for solving recurrence relations as bounds on running times; we then show 


how familiarity with these recurrences can guide the design of algorithms that 


improve over straightforward approaches to a number of basic problems, in- 


cluding the comparison of rankings, the computation of closest pairs of points 


in the plane, and the Fast Fourier Transform. Next we develop dynamic pro- 


gramming by starting with the recursive intuition behind it, and subsequently 


building up more and more expressive recurrence formulations through appli- 


cations in which they naturally arise. This chapter concludes with extended 


discussions of the dynamic programming approach to two fundamental prob- 


lems: sequence alignment, with applications in computational biology; and 


shortest paths in graphs, with connections to Internet routing protocols. Fi- 


nally, we cover algorithms for network ﬂow problems, devoting much of our 


focus in this chapter to discussing a large array of different ﬂow applications. 


To the extent that network ﬂow is covered in algorithms courses, students are 


often left without an appreciation for the wide range of problems to which it 


can be applied; we try to do justice to its versatility by presenting applications 


to load balancing, scheduling, image segmentation, and a number of other 


problems. 


Chapters 8 and 9 cover computational intractability. We devote most of 


our attention to NP-completeness, organizing the basic NP-complete problems 


thematically to help students recognize candidates for reductions when they 


encounter new problems. We build up to some fairly complex proofs of NP- 


completeness, with guidance on how one goes about constructing a difﬁcult 


reduction. We also consider types of computational hardness beyond NP- 


completeness, particularly through the topic of PSPACE-completeness. We 
Preface 


xix 


ﬁnd this is a valuable way to emphasize that intractability doesn’t end at 


NP-completeness, and PSPACE-completeness also forms the underpinning for 


some central notions from artiﬁcial intelligence—planning and game playing— 


that would otherwise not ﬁnd a place in the algorithmic landscape we are 


surveying. 


Chapters 10 through 12 cover three major techniques for dealing with com- 


putationally intractable problems: identiﬁcation of structured special cases, 


approximation algorithms, and local search heuristics. Our chapter on tractable 


special cases emphasizes that instances of NP-complete problems arising in 


practice may not be nearly as hard as worst-case instances, because they often 


contain some structure that can be exploited in the design of an efﬁcient algo- 


rithm. We illustrate how NP-complete problems are often efﬁciently solvable 


when restricted to tree-structured inputs, and we conclude with an extended 


discussion of tree decompositions of graphs. While this topic is more suit- 


able for a graduate course than for an undergraduate one, it is a technique 


with considerable practical utility for which it is hard to ﬁnd an existing 


accessible reference for students. Our chapter on approximation algorithms 


discusses both the process of designing effective algorithms and the task of 


understanding the optimal solution well enough to obtain good bounds on it. 


As design techniques for approximation algorithms, we focus on greedy algo- 


rithms, linear programming, and a third method we refer to as “pricing,” which 


incorporates ideas from each of the ﬁrst two. Finally, we discuss local search 


heuristics, including the Metropolis algorithm and simulated annealing. This 


topic is often missing from undergraduate algorithms courses, because very 


little is known in the way of provable guarantees for these algorithms; how- 


ever, given their widespread use in practice, we feel it is valuable for students 


to know something about them, and we also include some cases in which 


guarantees can be proved. 


Chapter 13 covers the use of randomization in the design of algorithms. 


This is a topic on which several nice graduate-level books have been written. 


Our goal here is to provide a more compact introduction to some of the 


ways in which students can apply randomized techniques using the kind of 


background in probability one typically gains from an undergraduate discrete 


math course. 


Use of the Book 


The book is primarily designed for use in a ﬁrst undergraduate course on 


algorithms, but it can also be used as the basis for an introductory graduate 


course. 


When we use the book at the undergraduate level, we spend roughly 


one lecture per numbered section; in cases where there is more than one 
xx 


Preface 


lecture’s worth of material in a section (for example, when a section provides 


further applications as additional examples), we treat this extra material as a 


supplement that students can read about outside of lecture. We skip the starred 


sections; while these sections contain important topics, they are less central 


to the development of the subject, and in some cases they are harder as well. 


We also tend to skip one or two other sections per chapter in the ﬁrst half of 


the book (for example, we tend to skip Sections 4.3, 4.7–4.8, 5.5–5.6, 6.5, 7.6, 


and 7.11). We cover roughly half of each of Chapters 11–13. 


This last point is worth emphasizing: rather than viewing the later chapters 


as “advanced,” and hence off-limits to undergraduate algorithms courses, we 


have designed them with the goal that the ﬁrst few sections of each should 


be accessible to an undergraduate audience. Our own undergraduate course 


involves material from all these chapters, as we feel that all of these topics 


have an important place at the undergraduate level. 


Finally, we treat Chapters 2 and 3 primarily as a review of material from 


earlier courses; but, as discussed above, the use of these two chapters depends 


heavily on the relationship of each speciﬁc course to its prerequisites. 


The resulting syllabus looks roughly as follows: Chapter 1; Chapters 4–8 


(excluding 4.3, 4.7–4.9, 5.5–5.6, 6.5, 6.10, 7.4, 7.6, 7.11, and 7.13); Chapter 9 


(brieﬂy); Chapter 10, Sections.10.1 and 10.2; Chapter 11, Sections 11.1, 11.2, 


11.6, and 11.8; Chapter 12, Sections 12.1–12.3; and Chapter 13, Sections 13.1– 


13.5. 


The book also naturally supports an introductory graduate course on 


algorithms. Our view of such a course is that it should introduce students 


destined for research in all different areas to the important current themes in 


algorithm design. Here we ﬁnd the emphasis on formulating problems to be 


useful as well, since students will soon be trying to deﬁne their own research 


problems in many different subﬁelds. For this type of course, we cover the 


later topics in Chapters 4 and 6 (Sections 4.5–4.9 and 6.5–6.10), cover all of 


Chapter 7 (moving more rapidly through the early sections), quickly cover NP- 


completeness in Chapter 8 (since many beginning graduate students will have 


seen this topic as undergraduates), and then spend the remainder of the time 


on Chapters 10–13. Although our focus in an introductory graduate course is 


on the more advanced sections, we ﬁnd it useful for the students to have the 


full book to consult for reviewing or ﬁlling in background knowledge, given 


the range of different undergraduate backgrounds among the students in such 


a course. 


Finally, the book can be used to support self-study by graduate students, 


researchers, or computer professionals who want to get a sense for how they 
Preface 


xxi 


might be able to use particular algorithm design techniques in the context of 


their own work. A number of graduate students and colleagues have used 


portions of the book in this way. 


Acknowledgments 


This book grew out of the sequence of algorithms courses that we have taught 


at Cornell. These courses have grown, as the ﬁeld has grown, over a number of 


years, and they reﬂect the inﬂuence of the Cornell faculty who helped to shape 


them during this time, including Juris Hartmanis, Monika Henzinger, John 


Hopcroft, Dexter Kozen, Ronitt Rubinfeld, and Sam Toueg. More generally, we 


would like to thank all our colleagues at Cornell for countless discussions both 


on the material here and on broader issues about the nature of the ﬁeld. 


The course staffs we’ve had in teaching the subject have been tremen- 


dously helpful in the formulation of this material. We thank our undergradu- 


ate and graduate teaching assistants, Siddharth Alexander, Rie Ando, Elliot 


Anshelevich, Lars Backstrom, Steve Baker, Ralph Benzinger, John Bicket, 


Doug Burdick, Mike Connor, Vladimir Dizhoor, Shaddin Doghmi, Alexan- 


der Druyan, Bowei Du, Sasha Evﬁmievski, Ariful Gani, Vadim Grinshpun, 


Ara Hayrapetyan, Chris Jeuell, Igor Kats, Omar Khan, Mikhail Kobyakov, 


Alexei Kopylov, Brian Kulis, Amit Kumar, Yeongwee Lee, Henry Lin, Ash- 


win Machanavajjhala, Ayan Mandal, Bill McCloskey, Leonid Meyerguz, Evan 


Moran, Niranjan Nagarajan, Tina Nolte, Travis Ortogero, Martin P´ 
al, Jon 


Peress, Matt Piotrowski, Joe Polastre, Mike Priscott, Xin Qi, Venu Ramasubra- 


manian, Aditya Rao, David Richardson, Brian Sabino, Rachit Siamwalla, Se- 


bastian Silgardo, Alex Slivkins, Chaitanya Swamy, Perry Tam, Nadya Travinin, 


Sergei Vassilvitskii, Matthew Wachs, Tom Wexler, Shan-Leung Maverick Woo, 


Justin Yang, and Misha Zatsman. Many of them have provided valuable in- 


sights, suggestions, and comments on the text. We also thank all the students 


in these classes who have provided comments and feedback on early drafts of 


the book over the years. 


For the past several years, the development of the book has beneﬁted 


greatly from the feedback and advice of colleagues who have used prepubli- 


cation drafts for teaching. Anna Karlin fearlessly adopted a draft as her course 


textbook at the University of Washington when it was still in an early stage of 


development; she was followed by a number of people who have used it either 


as a course textbook or as a resource for teaching: Paul Beame, Allan Borodin, 


Devdatt Dubhashi, David Kempe, Gene Kleinberg, Dexter Kozen, Amit Kumar, 


Mike Molloy, Yuval Rabani, Tim Roughgarden, Alexa Sharp, Shanghua Teng, 


Aravind Srinivasan, Dieter van Melkebeek, Kevin Wayne, Tom Wexler, and 
xxii 


Preface 


Sue Whitesides. We deeply appreciate their input and advice, which has in- 


formed many of our revisions to the content. We would like to additionally 


thank Kevin Wayne for producing supplementary material associated with the 


book, which promises to greatly extend its utility to future instructors. 


In a number of other cases, our approach to particular topics in the book 


reﬂects the infuence of speciﬁc colleagues. Many of these contributions have 


undoubtedly escaped our notice, but we especially thank Yuri Boykov, Ron 


Elber, Dan Huttenlocher, Bobby Kleinberg, Evie Kleinberg, Lillian Lee, David 


McAllester, Mark Newman, Prabhakar Raghavan, Bart Selman, David Shmoys, 


Steve Strogatz, Olga Veksler, Duncan Watts, and Ramin Zabih. 


It has been a pleasure working with Addison Wesley over the past year. 


First and foremost, we thank Matt Goldstein for all his advice and guidance in 


this process, and for helping us to synthesize a vast amount of review material 


into a concrete plan that improved the book. Our early conversations about 


the book with Susan Hartman were extremely valuable as well. We thank Matt 


and Susan, together with Michelle Brown, Marilyn Lloyd, Patty Mahtani, and 


Maite Suarez-Rivas at Addison Wesley, and Paul Anagnostopoulos and Jacqui 


Scarlott at Windfall Software, for all their work on the editing, production, and 


management of the project. We further thank Paul and Jacqui for their expert 


composition of the book. We thank Joyce Wells for the cover design, Nancy 


Murphy of Dartmouth Publishing for her work on the ﬁgures, Ted Laux for 


the indexing, and Carol Leyba and Jennifer McClain for the copyediting and 


proofreading. 


We thank Anselm Blumer (Tufts University), Richard Chang (University of 


Maryland, Baltimore County), Kevin Compton (University of Michigan), Diane 


Cook (University of Texas, Arlington), Sariel Har-Peled (University of Illinois, 


Urbana-Champaign), Sanjeev Khanna (University of Pennsylvania), Philip 


Klein (Brown University), David Matthias (Ohio State University), Adam Mey- 


erson (UCLA), Michael Mitzenmacher (Harvard University), Stephan Olariu 


(Old Dominion University), Mohan Paturi (UC San Diego), Edgar Ramos (Uni- 


versity of Illinois, Urbana-Champaign), Sanjay Ranka (University of Florida, 


Gainesville), Leon Reznik (Rochester Institute of Technology), Subhash Suri 


(UC Santa Barbara), Dieter van Melkebeek (University of Wisconsin, Madi- 


son), and Bulent Yener (Rensselaer Polytechnic Institute) who generously 


contributed their time to provide detailed and thoughtful reviews of the man- 


uscript; their comments led to numerous improvements, both large and small, 


in the ﬁnal version of the text. 


Finally, we thank our families—Lillian and Alice, and David, Rebecca, and 


Amy. We appreciate their support, patience, and many other contributions 


more than we can express in any acknowledgments here. 
Preface 


xxiii 


This book was begun amid the irrational exuberance of the late nineties, 


when the arc of computing technology seemed, to many of us, brieﬂy to pass 


through a place traditionally occupied by celebrities and other inhabitants of 


the pop-cultural ﬁrmament. (It was probably just in our imaginations.) Now, 


several years after the hype and stock prices have come back to earth, one can 


appreciate that in some ways computer science was forever changed by this 


period, and in other ways it has remained the same: the driving excitement 


that has characterized the ﬁeld since its early days is as strong and enticing as 


ever, the public’s fascination with information technology is still vibrant, and 


the reach of computing continues to extend into new disciplines. And so to 


all students of the subject, drawn to it for so many different reasons, we hope 


you ﬁnd this book an enjoyable and useful guide wherever your computational 


pursuits may take you. 


Jon Kleinberg 


´ 


Eva Tardos 


Ithaca, 2005 
This page intentionally left blank 
Chapter 1 


Introduction: Some 


Representative Problems 


1.1 A First Problem: Stable Matching 


As an opening topic, we look at an algorithmic problem that nicely illustrates 


many of the themes we will be emphasizing. It is motivated by some very 


natural and practical concerns, and from these we formulate a clean and 


simple statement of a problem. The algorithm to solve the problem is very 


clean as well, and most of our work will be spent in proving that it is correct 


and giving an acceptable bound on the amount of time it takes to terminate 


with an answer. The problem itself—the 
Stable Matching Problem
—has several 


origins. 


The Problem 


The Stable Matching Problem originated, in part, in 1962, when David Gale 


and Lloyd Shapley, two mathematical economists, asked the question: Could 


one design a college admissions process, or a job recruiting process, that was 


self-enforcing 
? What did they mean by this? 


To set up the question, let’s ﬁrst think informally about the kind of situation 


that might arise as a group of friends, all juniors in college majoring in 


computer science, begin applying to companies for summer internships. The 


crux of the application process is the interplay between two different types 


of parties: companies (the employers) and students (the applicants). Each 


applicant has a preference ordering on companies, and each company—once 


the applications come in—forms a preference ordering on its applicants. Based 


on these preferences, companies extend offers to some of their applicants, 


applicants choose which of their offers to accept, and people begin heading 


off to their summer internships. 
2 


Chapter 1 
Introduction: Some Representative Problems 


Gale and Shapley considered the sorts of things that could start going 


wrong with this process, in the absence of any mechanism to enforce the status 


quo. Suppose, for example, that your friend Raj has just accepted a summer job 


at the large telecommunications company CluNet. A few days later, the small 


start-up company WebExodus, which had been dragging its feet on making a 


few ﬁnal decisions, calls up Raj and offers him a summer job as well. Now, Raj 


actually prefers WebExodus to CluNet—won over perhaps by the laid-back, 


anything-can-happen atmosphere—and so this new development may well 


cause him to retract his acceptance of the CluNet offer and go to WebExodus 


instead. Suddenly down one summer intern, CluNet offers a job to one of its 


wait-listed applicants, who promptly retracts his previous acceptance of an 


offer from the software giant Babelsoft, and the situation begins to spiral out 


of control. 


Things look just as bad, if not worse, from the other direction. Suppose 


that Raj’s friend Chelsea, destined to go to Babelsoft but having just heard Raj’s 


story, calls up the people at WebExodus and says, “You know, I’d really rather 


spend the summer with you guys than at Babelsoft.” They ﬁnd this very easy 


to believe; and furthermore, on looking at Chelsea’s application, they realize 


that they would have rather hired her than some other student who actually 


is 
scheduled to spend the summer at WebExodus. In this case, if WebExodus 


were a slightly less scrupulous company, it might well ﬁnd some way to retract 


its offer to this other student and hire Chelsea instead. 


Situations like this can rapidly generate a lot of chaos, and many people— 


both applicants and employers—can end up unhappy with the process as well 


as the outcome. What has gone wrong? One basic problem is that the process 


is not self-enforcing—if people are allowed to act in their self-interest, then it 


risks breaking down. 


We might well prefer the following, more stable situation, in which self- 


interest itself prevents offers from being retracted and redirected. Consider 


another student, who has arranged to spend the summer at CluNet but calls 


up WebExodus and reveals that he, too, would rather work for them. But in 


this case, based on the offers already accepted, they are able to reply, “No, it 


turns out that we prefer each of the students we’ve accepted to you, so we’re 


afraid there’s nothing we can do.” Or consider an employer, earnestly following 


up with its top applicants who went elsewhere, being told by each of them, 


“No, I’m happy where I am.” In such a case, all the outcomes are stable—there 


are no further outside deals that can be made. 


So this is the question Gale and Shapley asked: Given a set of preferences 


among employers and applicants, can we assign applicants to employers so 


that for every employer 
E
, and every applicant 
A 
who is not scheduled to work 


for 
E
, at least one of the following two things is the case? 
1.1 A First Problem: Stable Matching 


3 


(i) 
E 
prefers every one of its accepted applicants to 
A
; or 


(ii) 
A 
prefers her current situation over working for employer 
E
. 


If this holds, the outcome is stable: individual self-interest will prevent any 


applicant/employer deal from being made behind the scenes. 


Gale and Shapley proceeded to develop a striking algorithmic solution to 


this problem, which we will discuss presently. Before doing this, let’s note that 


this is not the only origin of the Stable Matching Problem. It turns out that for 


a decade before the work of Gale and Shapley, unbeknownst to them, the 


National Resident Matching Program had been using a very similar procedure, 


with the same underlying motivation, to match residents to hospitals. Indeed, 


this system, with relatively little change, is still in use today. 


This is one testament to the problem’s fundamental appeal. And from the 


point of view of this book, it provides us with a nice ﬁrst domain in which 


to reason about some basic combinatorial deﬁnitions and the algorithms that 


build on them. 


Formulating the Problem 
To get at the essence of this concept, it helps to 


make the problem as clean as possible. The world of companies and applicants 


contains some distracting asymmetries. Each applicant is looking for a single 


company, but each company is looking for many applicants; moreover, there 


may be more (or, as is sometimes the case, fewer) applicants than there are 


available slots for summer jobs. Finally, each applicant does not typically apply 


to every company. 


It is useful, at least initially, to eliminate these complications and arrive at a 


more “bare-bones” version of the problem: each of 
n 
applicants applies to each 


of 
n 
companies, and each company wants to accept a 
single 
applicant. We will 


see that doing this preserves the fundamental issues inherent in the problem; 


in particular, our solution to this simpliﬁed version will extend directly to the 


more general case as well. 


Following Gale and Shapley, we observe that this special case can be 


viewed as the problem of devising a system by which each of 
n 
men and 


n 
women can end up getting married: our problem naturally has the analogue 


of two “genders”—the applicants and the companies—and in the case we are 


considering, everyone is seeking to be paired with exactly one individual of 


the opposite gender. 


1 


1 


Gale and Shapley considered the same-sex Stable Matching Problem as well, where there is only a 


single gender. This is motivated by related applications, but it turns out to be fairly diﬀerent at a 


technical level. Given the applicant-employer application we’re considering here, we’ll be focusing 


on the version with two genders. 
4 


Chapter 1 
Introduction: Some Representative Problems 


w 


m
 
w
 


m 


An instability: 
m 
and 
w
 


each prefer the other to 


their current partners. 


Figure 1.1 
Perfect matching 


S 
with instability 
(
m
, 
w 


 


)
. 


So consider a set 
M 
={
m 


1 


,..., 
m 


n 


} 
of 
n 
men, and a set 
W 
={
w 


1 


,..., 
w 


n 


} 


of 
n 
women. Let 
M 
× 
W 
denote the set of all possible ordered pairs of the form 


(
m
, 
w
)
, where 
m 
∈ 
M 
and 
w 
∈ 
W
.A 
matching S 
is a 
set 
of ordered pairs, each 


from 
M 
× 
W
, with the property that each member of 
M 
and each member of 


W 
appears in at most one pair in 
S
.A 
perfect matching S 


 


is a matching with 


the property that each member of 
M 
and each member of 
W 
appears in 
exactly 


one pair in 
S 


 


. 


Matchings and perfect matchings are objects that will recur frequently 


throughout the book; they arise naturally in modeling a wide range of algo- 


rithmic problems. In the present situation, a perfect matching corresponds 


simply to a way of pairing off the men with the women, in such a way that 


everyone ends up married to somebody, and nobody is married to more than 


one person—there is neither singlehood nor polygamy. 


Now we can add the notion of 
preferences 
to this setting. Each man 
m 
∈ 
M 


ranks 
all the women; we will say that 
m prefers w to w 


 


if 
m 
ranks 
w 
higher 


than 
w 


 


. We will refer to the ordered ranking of 
m 
as his 
preference list 
. We will 


not allow ties in the ranking. Each woman, analogously, ranks all the men. 


Given a perfect matching 
S
, what can go wrong? Guided by our initial 


motivation in terms of employers and applicants, we should be worried about 


the following situation: There are two pairs 
(
m
, 
w
) 
and 
(
m 


 


, 
w 


 


) 
in 
S 
(as 


depicted in Figure 1.1) with the property that 
m 
prefers 
w 


 


to 
w
, and 
w 


 


prefers 


m 
to 
m 


 


. In this case, there’s nothing to stop 
m 
and 
w 


 


from abandoning their 


current partners and heading off together; the set of marriages is not self- 


enforcing. We’ll say that such a pair 
(
m
, 
w 


 


) 
is an 
instability 
with respect to 
S
: 


(
m
, 
w 


 


) 
does not belong to 
S
, but each of 
m 
and 
w 


 


prefers the other to their 


partner in 
S
. 


Our goal, then, is a set of marriages with no instabilities. We’ll say that 


a matching 
S 
is 
stable 
if (i) it is perfect, and (ii) there is no instability with 


respect to 
S
. Two questions spring immediately to mind: 


. 


Does there exist a stable matching for every set of preference lists? 


. 


Given a set of preference lists, can we efﬁciently construct a stable 


matching if there is one? 


Some Examples 
To illustrate these deﬁnitions, consider the following two 


very simple instances of the Stable Matching Problem. 


First, suppose we have a set of two men, 
{
m
, 
m 


 


}
, and a set of two women, 


{
w
, 
w 


 


}
. The preference lists are as follows: 


m 
prefers 
w 
to 
w 


 


. 


m 


 


prefers 
w 
to 
w 


 


. 
1.1 A First Problem: Stable Matching 


5 


w 
prefers 
m 
to 
m 


 


. 


w 


 


prefers 
m 
to 
m 


 


. 


If we think about this set of preference lists intuitively, it represents complete 


agreement: the men agree on the order of the women, and the women agree 


on the order of the men. There is a unique stable matching here, consisting 


of the pairs 
(
m
, 
w
) 
and 
(
m 


 


, 
w 


 


)
. The other perfect matching, consisting of the 


pairs 
(
m 


 


, 
w
) 
and 
(
m
, 
w 


 


)
, would not be a stable matching, because the pair 


(
m
, 
w
) 
would form an instability with respect to this matching. (Both 
m 
and 


w 
would want to leave their respective partners and pair up.) 


Next, here’s an example where things are a bit more intricate. Suppose 


the preferences are 


m 
prefers 
w 
to 
w 


 


. 


m 


 


prefers 
w 


 


to 
w
. 


w 
prefers 
m 


 


to 
m
. 


w 


 


prefers 
m 
to 
m 


 


. 


What’s going on in this case? The two men’s preferences mesh perfectly with 


each other (they rank different women ﬁrst), and the two women’s preferences 


likewise mesh perfectly with each other. But the men’s preferences clash 


completely with the women’s preferences. 


In this second example, there are two different stable matchings. The 


matching consisting of the pairs 
(
m
, 
w
) 
and 
(
m 


 


, 
w 


 


) 
is stable, because both 


men are as happy as possible, so neither would leave their matched partner. 


But the matching consisting of the pairs 
(
m 


 


, 
w
) 
and 
(
m
, 
w 


 


) 
is also stable, for 


the complementary reason that both women are as happy as possible. This is 


an important point to remember as we go forward—it’s possible for an instance 


to have more than one stable matching. 


Designing the Algorithm 


We now show that there exists a stable matching for every set of preference 


lists among the men and women. Moreover, our means of showing this will 


also answer the second question that we asked above: we will give an efﬁcient 


algorithm that takes the preference lists and constructs a stable matching. 


Let us consider some of the basic ideas that motivate the algorithm. 


. 


Initially, everyone is unmarried. Suppose an unmarried man 
m 
chooses 


the woman 
w 
who ranks highest on his preference list and 
proposes 
to 


her. Can we declare immediately that 
(
m
, 
w
) 
will be one of the pairs in our 


ﬁnal stable matching? Not necessarily: at some point in the future, a man 


m 


 


whom 
w 
prefers may propose to her. On the other hand, it would be 
6 


Chapter 1 
Introduction: Some Representative Problems 


w 


m
 


m 


Woman 
w 
will become 


engaged to 
m 
if she 


prefers him to 
m

. 


Figure 1.2 
An intermediate 


state of the G-S algorithm 


when a free man 
m 
is propos- 


ing to a woman 
w
. 


dangerous for 
w 
to reject 
m 
right away; she may never receive a proposal 


from someone she ranks as highly as 
m
. So a natural idea would be to 


have the pair 
(
m
, 
w
) 
enter an intermediate state—
engagement 
. 


. 


Suppose we are now at a state in which some men and women are 
free
— 


not engaged—and some are engaged. The next step could look like this. 


An arbitrary free man 
m 
chooses the highest-ranked woman 
w 
to whom 


he has not yet proposed, and he proposes to her. If 
w 
is also free, then 
m 


and 
w 
become engaged. Otherwise, 
w 
is already engaged to some other 


man 
m 


 


. In this case, she determines which of 
m 
or 
m 


 


ranks higher 


on her preference list; this man becomes engaged to 
w 
and the other 


becomes free. 


. 


Finally, the algorithm will terminate when no one is free; at this moment, 


all engagements are declared ﬁnal, and the resulting perfect matching is 


returned. 


Here is a concrete description of the 
Gale-Shapley algorithm
, with Fig- 


ure 1.2 depicting a state of the algorithm. 


Initially all 
m 
∈ 
M 
and 
w 
∈ 
W 
are free 


While there is a man 
m 
who is free and hasn’t proposed to 


every woman 


Choose such a man 
m 


Let 
w 
be the highest-ranked woman in 
m
’s preference list 


to whom 
m 
has not yet proposed 


If 
w 
is free then 


(
m
, 
w
) 
become engaged 


Else 
w 
is currently engaged to 
m 


 


If 
w 
prefers 
m 


 


to 
m 
then 


m 
remains free 


Else 
w 
prefers 
m 
to 
m 


 


(
m
, 
w
) 
become engaged 


m 


 


becomes free 


Endif 


Endif 


Endwhile 


Return the set 
S 
of engaged pairs 


An intriguing thing is that, although the G-S algorithm is quite simple 


to state, it is not immediately obvious that it returns a stable matching, or 


even a perfect matching. We proceed to prove this now, through a sequence 


of intermediate facts. 
1.1 A First Problem: Stable Matching 


7 


Analyzing the Algorithm 


First consider the view of a woman 
w 
during the execution of the algorithm. 


For a while, no one has proposed to her, and she is free. Then a man 
m 
may 


propose to her, and she becomes engaged. As time goes on, she may receive 


additional proposals, accepting those that increase the rank of her partner. So 


we discover the following. 


(1.1) 
w remains engaged from the point at which she receives her ﬁrst 


proposal; and the sequence of partners to which she is engaged gets better and 


better (in terms of her preference list). 


The view of a man 
m 
during the execution of the algorithm is rather 


different. He is free until he proposes to the highest-ranked woman on his 


list; at this point he may or may not become engaged. As time goes on, he 


may alternate between being free and being engaged; however, the following 


property does hold. 


(1.2) 
The sequence of women to whom m proposes gets worse and worse (in 


terms of his preference list). 


Now we show that the algorithm terminates, and give a bound on the 


maximum number of iterations needed for termination. 


(1.3) 
The G-S algorithm terminates after at most n 


2 


iterations of the 
While 


loop. 


Proof. 
A useful strategy for upper-bounding the running time of an algorithm, 


as we are trying to do here, is to ﬁnd a measure of 
progress
. Namely, we seek 


some precise way of saying that each step taken by the algorithm brings it 


closer to termination. 


In the case of the present algorithm, each iteration consists of some man 


proposing (for the only time) to a woman he has never proposed to before. So 


if we let 
P
(
t 
) 
denote the set of pairs 
(
m
, 
w
) 
such that 
m 
has proposed to 
w 
by 


the end of iteration 
t 
, we see that for all 
t 
, the size of 
P
(
t 
+ 
1
) 
is strictly greater 


than the size of 
P
(
t 
)
. But there are only 
n 


2 


possible pairs of men and women 


in total, so the value of 
P
(
·
) 
can increase at most 
n 


2 


times over the course of 


the algorithm. It follows that there can be at most 
n 


2 


iterations. 


Two points are worth noting about the previous fact and its proof. First, 


there are executions of the algorithm (with certain preference lists) that can 


involve close to 
n 


2 


iterations, so this analysis is not far from the best possible. 


Second, there are many quantities that would not have worked well as a 


progress measure 
for the algorithm, since they need not strictly increase in each 
8 


Chapter 1 
Introduction: Some Representative Problems 


iteration. For example, the number of free individuals could remain constant 


from one iteration to the next, as could the number of engaged pairs. Thus, 


these quantities could not be used directly in giving an upper bound on the 


maximum possible number of iterations, in the style of the previous paragraph. 


Let us now establish that the set 
S 
returned at the termination of the 


algorithm is in fact a perfect matching. Why is this not immediately obvious? 


Essentially, we have to show that no man can “fall off” the end of his preference 


list; the only way for the 
While 
loop to exit is for there to be no free man. In 


this case, the set of engaged couples would indeed be a perfect matching. 


So the main thing we need to show is the following. 


(1.4) 
If m is free at some point in the execution of the algorithm, then there 


is a woman to whom he has not yet proposed. 


Proof. 
Suppose there comes a point when 
m 
is free but has already proposed 


to every woman. Then by (1.1), each of the 
n 
women is engaged at this point 


in time. Since the set of engaged pairs forms a matching, there must also be 


n 
engaged men at this point in time. But there are only 
n 
men total, and 
m 
is 


not engaged, so this is a contradiction. 


(1.5) 
The set S returned at termination is a perfect matching. 


Proof. 
The set of engaged pairs always forms a matching. Let us suppose that 


the algorithm terminates with a free man 
m
. At termination, it must be the 


case that 
m 
had already proposed to every woman, for otherwise the 
While 


loop would not have exited. But this contradicts (1.4), which says that there 


cannot be a free man who has proposed to every woman. 


Finally, we prove the main property of the algorithm—namely, that it 


results in a stable matching. 


(1.6) 
Consider an execution of the G-S algorithm that returns a set of pairs 


S. The set S is a stable matching. 


Proof. 
We have already seen, in (1.5), that 
S 
is a perfect matching. Thus, to 


prove 
S 
is a stable matching, we will assume that there is an instability with 


respect to 
S 
and obtain a contradiction. As deﬁned earlier, such an instability 


would involve two pairs, 
(
m
, 
w
) 
and 
(
m 


 


, 
w 


 


)
, in 
S 
with the properties that 


. 


m 
prefers 
w 


 


to 
w
, and 


. 


w 


 


prefers 
m 
to 
m 


 


. 


In the execution of the algorithm that produced 
S
, 
m
’s last proposal was, by 


deﬁnition, to 
w
. Now we ask: Did 
m 
propose to 
w 


 


at some earlier point in 
Have a problem with the book?

Report it to us using the bug icon. This will help us improve the service :)
book книга livre libro