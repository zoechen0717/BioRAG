Nature | www.nature.com | 1
Article
A vision–language foundation model for precision oncology
Jinxi Xiang1,6, Xiyue Wang1,6, Xiaoming Zhang2, Yinghua Xi1, Feyisope Eweje1, Yijiang Chen1, Yuchen Li1, Colin Bergstrom3, Matthew Gopaulchan1, Ted Kim1, Kun-Hsing Yu4, Sierra Willens3, Francesca Maria Olguin3, Jeffrey J. Nirschl2, Joel Neal3, Maximilian Diehn1, Sen Yang1✉ & Ruijiang Li1,5✉
Clinical decision-making is driven by multimodal data, including clinical notes and pathological characteristics. Artificial intelligence approaches that can effectively integrate multimodal data hold significant promise in advancing clinical care1,2. However, the scarcity of well-annotated multimodal datasets in clinical settings has hindered the development of useful models. In this study, we developed the Multimodal transformer with Unified maSKed modeling (MUSK), a vision–language foundation model designed to leverage large-scale, unlabelled, unpaired image and text data. MUSK was pretrained on 50 million pathology images from 11,577 patients and one billion pathology-related text tokens using unified masked modelling. It was further pretrained on one million pathology image–text pairs to efficiently align the vision and language features. With minimal or no further training, MUSK was tested in a wide range of applications and demonstrated superior performance across 23 patch-level and slide-level benchmarks, including image-to-text and text-to-image retrieval, visual question answering, image classification and molecular biomarker prediction. Furthermore, MUSK showed strong performance in outcome prediction, including melanoma relapse prediction, pan-cancer prognosis prediction and immunotherapy response prediction in lung and gastro-oesophageal cancers. MUSK effectively combined complementary information from pathology images and clinical reports and could potentially improve diagnosis and precision in cancer therapy.
Clinical decision-making is a complex process that involves information obtained from multiple data modalities. In clinical practice, physicians do not rely on a single data source to make diagnosis and treatment decisions. Instead, they incorporate information from multiple sources, including patient demographics, medical history, imaging findings and the pathological characteristics of the disease. Therefore, making accurate diagnosis and treatment decisions requires the synthesis of information from multimodal data. Given the complexity of these tasks, artificial intelligence (AI) approaches that can effectively integrate multimodal data hold significant promise to advance clinical care1–5. Foundation models represent a new frontier in medical AI research and development6,7. These models are pretrained on massive, diverse datasets and can be applied to numerous downstream tasks with minimal or no further training8–13. This has significant advantages over the traditional approach, which requires training a new model for every new task. However, a major hurdle in the development of multimodal AI models is the scarcity of well-annotated datasets, especially in the clinical setting. Recent efforts have been made to develop vision–language foundation models for medicine14, particularly in the field of pathology15–17.
Although the initial results are promising, several important considerations could limit their potential clinical impact. First, these studies used off-the-shelf foundation models based on contrastive learning18, which requires paired image–text data for pretraining. Although the scale of data is impressive with approximately 0.2–1.2 million image–text pairs, it is still far below the billions of data points used for training natural vision–language models19. In addition, it remains unclear whether this scale is sufficient to fully capture the diversity of the entire disease spectrum. Second, previous studies have focused on relatively simple tasks, such as image classification or image and text retrieval, with the intended applications for cancer detection and diagnosis. However, prediction of treatment response and outcomes using multimodal foundation models has not yet been demonstrated. This is a much more challenging problem but has significant implications for guiding treatment decisions in precision medicine20. Here, we present a new vision–language foundation model based on Multimodal transformer with Unified maSKed modeling (MUSK) for pretraining. Motivated by the success of multimodal learning of natural image–text data21, we introduce pathology-specific and general methodology adaptations to the MUSK approach for building
https://doi.org/10.1038/s41586-024-08378-w
Received: 28 May 2024
Accepted: 8 November 2024
Published online: xx xx xxxx
Check for updates
1Department of Radiation Oncology, Stanford University School of Medicine, Stanford, CA, USA. 2Department of Pathology, Stanford University School of Medicine, Stanford, CA, USA. 3Department of Medicine (Oncology), Stanford University School of Medicine, Stanford, CA, USA. 4Department of Biomedical Informatics, Harvard Medical School, Boston, MA, USA. 5Stanford Institute for Human-Centered Artificial Intelligence, Stanford, CA, USA. 6These authors contributed equally: Jinxi Xiang, Xiyue Wang. ✉e-mail: seny@stanford.edu; rli2@stanford.edu


2 | Nature | www.nature.com
Article
high-performance foundation models for precision oncology. MUSK pretraining leverages large-scale, unlabelled, unpaired data with 50 million pathology images and one billion text tokens (Fig. 1a). The pathology images used for masked pretraining originated from 11,577 patients, representing 33 tumour types. We performed extensive evaluation on a wide range of downstream tasks, including image and text retrieval, visual question answering (VQA), image classification and molecular
biomarker prediction. MUSK achieved superior performance over the state-of-the-art foundation models on 23 patch-level and slide-level benchmarks (Fig. 1b). Furthermore, MUSK was evaluated on multimodal clinical reports and image data from more than 8,000 patients and showed strong performance in predicting clinical outcomes, including melanoma relapse, pan-cancer prognosis and immunotherapy response predictions.
a Vision–language foundation model pretraining
MUSK (Multimodal transformer)
Masked data modelling
Vision expert
Language expert
Shared self-attention
Switch by modality
Image tokens
Text tokens
Represents masked tokens
Step 2: Contrastive learning using image–text pairs
General-purpose clinical applications
MUSK-V
MIL aggregator
Pathology reports
MUSK-L
MLP
Gastro-oesophageal cancer immunotherapy (n = 101 patients)
Lung cancer immunotherapy (n = 118 patients)
Prognosis prediction (n = 6,602 patients across 16 cancers)
Melanoma relapse prediction (n = 1,342 patients) MUSK-V
MUSK-V
A histopathological image....
Structured reports
Whole-slide image
Segment and crop
(i) Text-to-image/image-to-text retrieval
MUSK
MUSK
A histopathological image of an invasive tissue from breast. ...
Results
MUSK
(iii) Zero-shot image classification
(ii) Visual question answering
MUSK
Image
Questions
Answer outputs
Q1: What ...? Q2: Are ...? Q3: Where ...?
1 billion text tokens extracted from 1 million articles
MUSK Results
(iv) Supervised image classification
(1) Cancer detection and diagnosis
(2) Outcome prediction
50 million image patches extracted from 32,898 slides
Performance evaluation
Vision
expert
Self
attention
Language
expert
Self
attention
Modality alignment
Shared
Image–text pair
Image encoder
Performance evaluation
Step 1: Unified masked learning using large-scale unpaired images and texts
50 million image patches
1 billion text tokens
1 million image–text pairs
Total pretraining data
A histopathological image of [CLS]
Text encoder
0.5
0.6
0.7
0.8
Melanoma
relapse
Pan-cancer
prognosis
Lungcancer
immunotherapy
Gastriccancer
immunotherapy
PLIP QUILT-1M BiomedCLIP CONCH MUSK
AUC
GPT-4
PathVQA BookSet (T2I)
BookSet (I2T)
PathMMU (T2I)
PathMMU (I2T)
PanNuke
PatchCam
SkinCancer NCT-CRC UniToPatho
SICAPv2
WSSS4LUAD
Osteo
LC25000
RenalCell
BRACS (3CLS)
BRACS (6CLS)
0.67 0.55 0.52
0.25
0.23
0.72
0.69
0.43 0.92 0.31
0.73
0.94
0.93
0.92
0.86
0.71
0.58
0.75 0.76
0.75
0.36
0.36
0.84
0.8
0.59 0.97 0.39
0.81
0.99
0.99
0.99
0.93
0.79
0.67
PLIP QUILT-1M (ViT-b16) BiomedCLIP CONCH MUSK
Vision–language Vision
b
Morphologically, basal-like breast cancers present as invasive ductal carcinomas with high histologic grades and brisk mitotic activity.
Fig. 1 | Data curation, model development and evaluation. a, MUSK model pretraining. We developed a vision–language foundation model built upon a multimodal transformer architecture as the network backbone. Model pretraining consisted of two sequential phases. First, MUSK was pretrained on 50 million pathology images and one billion pathology-related text tokens. The images originated from nearly 33,000 whole-slide histopathology scans from 11,577 patients, representing 33 tumour types. Adapted from BEiT3 (ref. 21) architecture, the MUSK model consisted of shared self-attention blocks and two independent experts for vision and language inputs; pretraining was achieved using masked modelling. Second, MUSK was pretrained on one
million image–text pairs from the model QUILT-1M using contrastive learning for multimodal alignment. b, General-purpose clinical application. Once the pretraining is complete, MUSK can be used for various downstream tasks with minimal or no further training. Importantly, we evaluated MUSK using whole-slide images and clinical reports for outcome prediction, including relapse, prognosis and immunotherapy response predictions. MUSK substantially improved upon state-of-the-art vision–language foundation models, including PLIP15, QUILT1M46, BiomedCLIP47 and CONCH16. The graphics of reports, melanoma, prognosis, lung cancer and gastro-oesophageal cancer in b were created using BioRender (https://biorender.com).MIL,multiple instance learning.


Nature | www.nature.com | 3
Zero-shot cross-modal retrieval
A key feature of a foundation model is its ability to perform downstream tasks without further training, that is, zero-shot learning capability15,16. By learning an aligned latent embedding space for visual and language representations, MUSK can retrieve relevant texts based on an image query and vice versa. We evaluated MUSK for zero-shot cross-modal retrieval on two benchmark datasets, BookSet22 and PathMMU23, with 4,265 and 7,774 image–text pairs, respectively. MUSK achieved superior performance over the seven other foundation models in both image-to-text and text-to-image retrieval tasks (Fig. 2a and Supplementary Tables 1 and 2). On the PathMMU dataset, MUSK outperformed the second-best model (CONCH) with 34.4% (95% confidence interval (CI): 33.4–35.5%) versus 27.3% (95% CI: 26.4–28.3%) for image-to-text retrieval for Recall@50 (that is, recall
rate of the top 50 retrieval candidates). Similarly, on the BookSet dataset, MUSK outperformed the second-best model (CONCH) with 74.8% (95% CI: 73.6–75.9%) versus 71.3% (95% CI: 70.0–72.6%) for Recall@50. We observed similar patterns for text-to-image retrieval tasks, with MUSK surpassing the second-best model with an improvement at Recall@50 of 4.0% and 7.5% in absolute terms. These results demonstrate the strong zero-shot learning capability of MUSK.
Visual question answering
In addition to cross-modal retrieval, another common vision–language task is VQA, which uses the input of pathology images and accompanying textual questions to generate an answer. Existing approaches require the design of sophisticated network models, specifically for this task24–27. By contrast, MUSK is a general-purpose vision–language
Cross-modal retrieval
a
Visual question answering
b
PLIP: joints MUSK: malignant melanoma
PLIP: gastrointestinal system MUSK: prostate
PLIP: endocrine MUSK: haematologic
PLIP: cardiovascular MUSK: pancreas
PLIP: connective tissue MUSK: the alveoli
PLIP: yes MUSK: yes
Some visual answering results:
Question: Did microscopic appearance of medulloblastoma show mostly small, blue, primitive-appearing tumour cells?
Question: What are filled with a characteristic foamy acellular exudate?
Question: What is presented? Question: What does it show?
Question: Where is this from?
Recall@1 Recall@10 Recall@50
0
10
20
30
40
Recall
Image-to-text retrieval on PathMMU
P < 0.0001
P < 0.0001
P < 0.0001
Recall@1 Recall@10 Recall@50
0
10
20
30
40
Recall
P < 0.0001
P < 0.0001
P < 0.0001
Text-to-image retrieval on PathMMU
Recall@1 Recall@10 Recall@50
0
20
40
60
80
Recall
Recall@1 Recall@10 Recall@50
0
20
40
60
80
Recall
P = 0.004
P = 0.013
P < 0.0001
Image-to-text retrieval on BookSet
P < 0.0001
P = 0.03
P < 0.0001
Text-to-image retrieval on BookSet
Question: What is presented?
50
55
60
65
70
75
80
BAN
MEVF
VisualBERT
Trap-VQA
K-PathVQA PLIP
QUILT-1M
BiomedCLIP
CONCH
MUSK
Accuracy (%)
Task-specific models Foundation models P < 0.0001
CLIP (ViT-b16) CLIP (ViT-b32) PLIP QUILT-1M (ViT-b16)
QUILT-1M (ViT-b32) BiomedCLIP CONCH MUSK
CLIP (ViT-b16) CLIP (ViT-b32) PLIP QUILT-1M (ViT-b16)
QUILT-1M (ViT-b32) BiomedCLIP CONCH MUSK
CLIP (ViT-b16) CLIP (ViT-b32) PLIP QUILT-1M (ViT-b16)
QUILT-1M (ViT-b32) BiomedCLIP CONCH MUSK
CLIP (ViT-b16) CLIP (ViT-b32) PLIP QUILT-1M (ViT-b16)
QUILT-1M (ViT-b32) BiomedCLIP CONCH MUSK
Fig. 2 | Cross-modal retrieval and VQA. a, Zero-shot image-to-text and text-to-image retrieval. MUSK consistently outperformed the existing foundation models across different recall levels on BookSet and PathMMU. The two-sided Wilcoxon signed-rank test was used to assess the statistical differences between MUSK and the second-best model (CONCH). Visual examples are shown in Supplementary Fig. 4. b, VQA. MUSK substantially outperformed the existing foundation models in the PathVQA benchmark
dataset. Notably, MUSK improved the accuracy by 7% over that of the best-performing model (K-PathVQA) specifically trained for VQA. Some examples of the results are shown for MUSK and PLIP models. The two-sided Mann–Whitney U-test was used to evaluate the statistical significance. For VQA task-specific models, no CIs were reported in the original papers. In a and b, data in foundation models are represented as the mean with 95% CIs estimated using the bootstrap method (n = 1,000 replicates).


4 | Nature | www.nature.com
Article
foundation model that can perform VQA with minimal training (Fig. 1b and Supplementary Fig.3). We evaluated the performance on the PathVQA28 dataset, which comprised 32,799 questions derived from 4,998 pathology images. MUSK achieved an accuracy of 73.2% (95% CI: 72.174.4%), significantly outperforming other vision–language foundation models, including PLIP15, QUILT-1M, BiomedCLIP and CONCH (Fig. 2b). Notably, MUSK surpassed the best-performing model, K-PathVQA27, specifically designed for VQA purposes (accuracy: 68.9%), highlighting the advantage of building powerful foundation models.
Image retrieval and classification
Although MUSK was developed as a multimodal foundation model, it can also serve as a stand-alone image encoder. Here, we demonstrate the vision capability of MUSK across various image-based tasks, including image retrieval and classification. We evaluated the performance of zero-shot image retrieval on UniToPatho29 and BRACS30 datasets. In both datasets, MUSK outperformed the other foundation models across all evaluation metrics (Extended Data Fig. 1a,b and Supplementary Table 3). For example, MUSK exceeded CLIP by 22.3%, PLIP by 8.6% and CONCH by 2.5% in terms of mMV@5 (that is, the accuracy of the top 5 majority votes) on the BRACS dataset. For image classification, we first evaluated the performance of the model for zero-shot learning on four benchmark datasets: PatchCamelyon31, SkinCancer32, PanNuke33 and UniToPatho29. Despite the challenging task of distinguishing multiple classes and the lack of any training data, MUSK still achieved promising performance for zero-shot image classification (Fig. 3a and Supplementary Table 4), surpassing the
second-best model (CONCH, BiomedCLIP or QUILT-1M depending on the dataset) by 10.5%, 27.5%, 7.3%, and 10.1%. We then assessed the capability of MUSK for few-shot image classification, that is, using only a few samples to fine-tune the pretrained model. This can be useful when the amount of training data is small, either because it is practically difficult to annotate enough samples or because the prevalence of disease is low. We performed a comprehensive evaluation of the model for few-shot image classification using 12 benchmark datasets. These datasets contain expert-annotated histopathology images of normal samples and malignancies from various tissues/organs, such as skin, lung, colon, breast, prostate, kidney and lymph nodes. Across all 12 datasets29–39, MUSK showed the highest accuracy for ten-shot image classification compared with the other foundation models (Fig. 3b, Extended Data Fig. 2a and Supplementary Table 5). Notably, the increase in classification accuracy was highest in the most challenging tasks, for which the other models did not perform well. For instance, in the UniToPatho dataset, MUSK achieved an increase of 9.8% over the second-best model. Furthermore, we evaluated the model for one-, two-, four- and eight-shot image classification with even fewer training samples and obtained similar results (Extended Data Fig. 1c). This indicates that MUSK is a robust and label-efficient vision encoder for image classification. Finally, we evaluated the model for supervised image classification using all the available training data in each of the 12 benchmark datasets. MUSK achieved an average accuracy of 88.2%, outperforming other foundation models, including CLIP, PLIP, QUILT-1M, BiomedCLIP and CONCH by margins of 17.5%, 9.1%, 11.7%, 11% and 2.2%, respectively (Extended Data Fig. 2b and Supplementary Table 6). Visualizations of
UniToPatho
b
SkinCancer PatchCamelyon PanNuke
0
20
40
60
80
100 CLIP (ViT-b16) CLIP (ViT-b32)
PLIP QUILT-1M (ViT-b16)
QUILT-1M (ViT-b32) BiomedCLIP
CONCH MUSK
P < 0.0001
P < 0.0001
P < 0.0001
P < 0.0001
Balanced accuracy
BRACS(3-cls)
UniToPatho
BRACS(6-cls)
SICAPv2
PCam
LC25000
PanNuke
RenalCell
SkinCancer
NCT-CRC
Osteo
WSSS4LUAD
20
40
60
80
100
PLIP BiomedCLIP QUILT-1M-B-16 CONCH MUSK
P = 0.43
P = 0.002 P = 0.002
P = 0.01
P = 0.006
P = 0.002
P = 0.23 P = 0.01 P = 0.06 P = 0.04 P = 0.006
P = 0.002
Zero-shot image classification
Ten-shot image classification on 12 benchmarks
a
Fig. 3 | Patch-level image classification. a, Zero-shot image classification. MUSK consistently outperformed seven alternative foundation models when evaluated on the UniToPatho, SkinCancer, PatchCamelyon and PanNuke benchmark datasets, with P < 0.0001.b, Ten-shot image classification. MUSK consistently outperformed the other foundation models across the 12 benchmark datasets.
Two-sided Wilcoxon signed-rank tests were used to calculate the statistical differences between MUSK and the top-performing alternative model. Data are presented as means and 95% CIs (error bars). These intervals were estimated using the bootstrap method (n = 1,000 replicates) (a) or calculated from n = 10 independent experiments (b).


Nature | www.nature.com | 5
image embeddings produced by different models further highlight the robustness of the feature representation capabilities of MUSK (Supplementary Fig. 5). These results demonstrate that MUSK provides a powerful approach to learn more effective image representations for pathology classification.
Molecular biomarker prediction
Molecular biomarkers, such as protein expression and gene mutation, are critical components of precision oncology that can directly inform targeted therapies40. In this study, we evaluated the performance of MUSK against five state-of-the-art pathology foundation models for predicting molecular biomarkers from slide-level histopathology images. Specifically, we assessed the models for predicting receptor status in breast cancer and isocitrate dehydrogenase (IDH) mutation status in brain tumours using the publicly available Early Breast Cancer Core-Needle Biopsy (BCNB)41 and Medical University of Vienna (MUV-IDH)42 datasets, respectively. MUSK achieved significantly higher performance than other pathology foundation models, including PLIP15, UNI11, GigaPath10, Virchow12 and CONCH16, in predicting oestrogen receptor (ER), progesterone receptor (PR), human epidermal growth factor receptor 2 (HER2) status and IDH mutation status (MannWhitney U-test, P < 0.05; Extended Data Fig. 3a and Supplementary Table 7). For instance, our model achieved an area under the receiver operating characteristic curve (AUC) of 0.826 (95% CI: 0.813–0.839) for predicting HER2 status, which is a significant improvement over the leading comparison methods: GigaPath (0.786; 95% CI: 0.756–0.817) and CONCH (0.771; 95% CI: 0.745–0.796); P = 0.008.
Melanoma relapse prediction
Melanoma is the most serious form of skin cancer and has a relatively high likelihood of relapse that can lead to death. The accurate prediction of relapse after curative-intent surgery may inform personalized treatment strategies43. For instance, patients at high risk of relapse should receive adjuvant systemic therapy, whereas those at low risk of relapse may avoid the toxicity associated with the drugs. Traditional risk factors, such as tumour thickness and presence of ulceration, do not accurately predict an individual patient’s relapse44. To address this unmet need, we developed a multimodal approach based on MUSK to predict the risk of relapse in melanoma. We used the VisioMel dataset45, which includes clinical reports and whole-slide images (WSIs) of diagnostic haematoxylin and eosin (H&E) slides, as well as 5-year follow-up data for 1,342 patients with melanoma. Compared with existing vision–language foundation models, MUSK achieved the highest AUC of 0.833 (95% CI: 0.818–0.847) for predicting 5-year relapse, significantly outperforming PLIP, QUILT-1M, BiomedCLIP and CONCH (Extended Data Fig. 4a,b). We then conducted ablation experiments using single-modal inputs on the MUSK model (Extended Data Fig. 4d). The results show that a model based on clinical reports or images alone has lower accuracy in predicting relapse. By combining the complementary information obtained from two different data modalities, MUSK further improved the accuracy of relapse prediction, highlighting the power of our multimodal approach. To be clinically useful, a prognostic model should be highly sensitive in predicting relapse to minimize the risk of under-treatment. Thus, we evaluated the performance of the model at a predetermined sensitivity threshold of 90% (Extended Data Fig.4c). The MUSK model achieved a substantially higher specificity than the other foundation models, with an improvement of about 12% (P = 0.0079). The clinical implication is that our model may spare more patients from toxic but unnecessary adjuvant therapy. Finally, visualization of the model’s prediction revealed that MUSK could automatically uncover relevant pathological features for predicting relapse (Extended Data Fig. 4e and Supplementary Fig. 6).
Pan-cancer prognosis prediction
Having demonstrated the effectiveness of our approach for predicting relapse, specifically in melanoma, we next evaluated the model for its ability to predict prognosis broadly in a pan-cancer setting. To do this, we collected diagnostic H&E WSIs, associated pathology reports and follow-up data from The Cancer Genome Atlas (TCGA), encompassing a total of 7,927 WSIs from 6,602 patients across 16 major cancer types. We trained a multimodal prognostic model for each cancer type and then evaluated its performance in predicting disease-specific survival. Across all 16 cancer types, MUSK consistently outperformed the clinical risk factors and state-of-the-art foundation models for prognosis prediction. On average, MUSK achieved a concordance index (c-index) of 0.747, significantly above the c-index of 0.645 (P < 0.0001) for the overall stage; 0.668 (P < 0.0001), 0.672 (P < 0.0001), 0.668 (P < 0.0001) and 0.684 (P < 0.0001) for the multimodal foundation models PLIP15, QUILT-1M46, BiomedCLIP47 and CONCH16, respectively; and 0.681 (P < 0.0001), 0.681 (P < 0.0001) and 0.672 (P < 0.0001) for the pathology foundation models UNI11, GigaPath10 and Virchow12, respectively (two-sided Mann–Whitney U-test; Fig. 4a, Extended Data Fig. 3c, Supplementary Fig. 7 and Supplementary Table 7). The best prediction was achieved for renal cell carcinoma, with a c-index of 0.887 (95% CI: 0.854–0.920). In addition, MUSK achieved high performance for prognosis prediction in breast invasive carcinoma, colorectal adenocarcinoma, low-grade glioma and endometrial carcinoma, with a c-index above 0.8. We assessed the ability of the MUSK model to stratify patients for disease-specific survival using Kaplan–Meier analysis. Our results demonstrate a significant stratification of low-risk and high-risk patients with distinct survival outcomes (log-rank test, P < 0.001) across 16 cancer types (Fig. 4a). Strikingly, the model achieved a hazard ratio (HR) of greater than 30 in renal cell carcinoma, with a 10-year survival rate of 95.3% versus 48.3% in the low-risk and high-risk groups. We further conducted multivariable Cox regression analysis and confirmed that the MUSK-based risk score was a significant prognostic factor across all 16 cancer types, independent of clinical risk variables, including age, sex, stage and tumour grade (Supplementary Fig. 8). We conducted ablation experiments on the MUSK model by training the image-only and text-only models for prognosis prediction. These models showed reasonable performance with average c-indices of 0.654 (P < 0.0001) and 0.673 (P < 0.0001), respectively. Of note, the multimodal MUSK model consistently outperformed the prognostic models with single-modal inputs across all 16 cancer types (Fig. 4b), with a significantly higher c-index of 0.746. These results demonstrate that MUSK can effectively integrate the complementary information of multimodal image and text data for prognosis prediction across cancer types.
Immunotherapy response prediction
Immunotherapy, specifically immune checkpoint inhibitors (ICIs), has transformed the treatment landscape in oncology and offers the potential for long-term durable benefits. However, only approximately 20% of patients respond to and benefit from ICIs48,49. It is critical to identify which patients will benefit from ICIs given the toxicity and financial burden of these treatments. Existing biomarkers, such as tumour programmed cell death ligand 1(PD-L1) expression and tumour mutation burden, have limited predictive power in distinguishing responders from non-responders50,51. There is an unmet need for a more accurate prediction of immunotherapy response. Here, we collected multimodal datasets, including pretreatment H&E slides, associated pathology reports, therapy response and follow-up data for 118 patients with advanced non-small cell lung cancer (NSCLC) treated with ICIs. We evaluated the MUSK model for predicting two clinical endpoints: objective response


6 | Nature | www.nature.com
Article
and progression-free survival (PFS). The patients were classified as responders (complete or partial response) or non-responders (stable or progressive disease). For predicting response, MUSK achieved an AUC of 0.768 (95% CI: 0.724–0.812), which was significantly higher than that of existing biomarkers, such as tumour PD-L1 expression, with an AUC of 0.606 (95% CI: 0.492–0.699; P < 0.0001). MUSK also outperformed the models
trained using other multimodal foundation methods, such as PLIP, QUILT-1M, BiomedCLIP and CONCH, with AUC ranging from 0.636 to 0.692 (Fig. 5a). Similarly, for predicting PFS, MUSK demonstrated a significant improvement over existing biomarkers, with a c-index of 0.705 (95% CI: 0.677–0.732) compared with 0.574 (95% CI: 0.447–0.691; P < 0.0001) for tumour PD-L1 expression (Supplementary Fig. 10a). MUSK achieved a significantly higher performance for PFS prediction
a
BLCA
BRCA
CESC
COADREAD
ESCA GBM
HNSC LGG
LIHC
LUAD
LUSC
PAAD RCC
SKCM
STAD
UCEC
Overall
0.4
0.5
0.6
0.7
0.8
0.9
1.0
c-index
All cancer types
b
0 20 40 60 80 100
0
0.2
0.4
0.6
0.8
1.0
Survival probability
HR: 2.23 (1.56–3.19) P < 0.0001
BLCA (N = 368)
0 20 40 60 80 100
0
0.2
0.4
0.6
0.8
1.0
HR: 3.69 (2.23–6.12) P < 0.0001
BRCA (N = 1,007)
0 50 100 150 200
0
0.2
0.4
0.6
0.8
1.0
HR: 3.39 (1.76–6.52) P = 0.0001
CESC (N = 258)
0 50 100 150
0
0.2
0.4
0.6
0.8
1.0
HR: 16.60 (5.99–45.97) P < 0.0001
COADREAD (N = 392)
High risk Low risk
High risk Low risk
High risk Low risk
High risk Low risk
0 10 20 30 40 50
0
0.2
0.4
0.6
0.8
1.0
Survival probability
HR: 6.17 (2.65–14.37) P < 0.0001
ESCA (N = 134)
0 20 40 60
0
0.2
0.4
0.6
0.8
1.0
HR: 1.59 (1.34–1.88) P < 0.0001
GBM (N = 347)
0 50 100 150
0
0.2
0.4
0.6
0.8
1.0
HR: 2.92 (1.91–4.46) P < 0.0001
HNSC (N = 378)
0 50 100 150 200
0
0.2
0.4
0.6
0.8
1.0
HR: 7.46 (4.34–12.83) P < 0.0001
LGG (N = 480)
0 20 40 60 80
0
0.2
0.4
0.6
0.8
1.0
Survival probability
HR: 2.72 (1.63–4.54) P < 0.0001
LIHC (N = 332)
0 50 100 150 200 250
0
0.2
0.4
0.6
0.8
1.0
HR: 2.62 (1.76–3.88) P < 0.0001
LUAD (N = 419)
0 20 40 60 80 100
0
0.2
0.4
0.6
0.8
1.0
HR: 2.35 (1.47–3.77) P = 0.0003
LUSC (N = 405)
0 10 20 30 40 50
0
0.2
0.4
0.6
0.8
1.0
HR: 2.10 (1.33–3.31) P = 0.0011
PAAD (N = 173)
0 50 100 150 Time (months) Time (months) Time (months) Time (months)
0
0.2
0.4
0.6
0.8
1.0
Survival probability
HR: 36.83 (13.62–99.61) P < 0.0001
RCC (N = 716)
0 50 100 150 200
0
0.2
0.4
0.6
0.8
1.0
HR: 1.95 (1.41–2.69) P < 0.0001
SKCM (N = 350)
0 25 50 75 100 125
0
0.2
0.4
0.6
0.8
1.0
HR: 3.05 (1.96–4.74) P < 0.0001
STAD (N = 353)
0 50 100 150 200
0
0.2
0.4
0.6
0.8
1.0
HR: 8.65 (3.92–19.07) P < 0.0001
UCEC (N = 490)
P < 0.0001
P < 0.0001
MUSK (image) MUSK (report) MUSK (image + report)
Fig. 4 | Prognosis prediction across 16 cancer types. a, Kaplan–Meier analyses showed that MUSK can significantly stratify patients for disease-specific survival across 16 cancer types, with HRs ranging from 1.59 for glioblastoma multiforme to 36.83 for renal cell carcinoma. The two-sided log-rank test was used to compare the survival differences between the high-risk and low-risk groups (cut-off: median). b, The multimodal MUSK model significantly improved prognosis prediction over models based on clinical reports or WSIs alone, as shown in the overall bars (P < 0.0001). The overall bars represent the average performance across 16 projects. Bladder urothelial carcinoma (BLCA), breast invasive carcinoma (BRCA), cervical squamous cell carcinoma and endocervical
adenocarcinoma (CESC), colorectal adenocarcinoma rectal adenocarcinoma (COADREAD), esophageal carcinoma (ESCA), glioblastoma multiforme (GBM), head and neck squamous cell carcinoma (HNSC), low-grade glioma (LGG), liver hepatocellular carcinoma (LIHC), lung adenocarcinoma (LUAD), lung squamous cell carcinoma (LUSC), pancreatic adenocarcinoma (PAAD), renal cell carcinoma (RCC), skin cutaneous melanoma (SKCM), stomach adenocarcinoma (STAD) and uterine corpus endometrial carcinoma (UCEC). In b, data are represented as the mean with standard deviation calculated using five-fold cross-validation experiments. The two-sided Mann–Whitney U-test was used to assess the statistical significance between MUSK and the comparison methods.


Nature | www.nature.com | 7
than the existing pathology foundation models, such as UNI, GigaPath and Virchow, with a c-index between 0.580 and 0.599 (Extended Data Fig. 3b and Supplementary Table 7). Compared with alternative
multimodal approaches, MUSK also showed superior performance for PFS prediction over PLIP, QUILT-1M, BiomedCLIP and CONCH, with a c-index in the range of 0.601–0.640 (Fig. 5a).
c
ab
d
All patients PD-L1 TPS = 0 PD-L1 TPS ≥ 1 EGFR mutant
WSI Heat map ROI 1 ROI 2
Non-responder Responder
5 mm 5 mm
2 mm 2 mm
200 μm 200 μm
200 μm 200 μm
12
12
12 12
1
2 1
2
HR: 2.54 (1.66–3.90) P < 0.0001
0
0.25
0.50
0.75
1.00
0 20 40 60 80 Time (months)
Progression-free survival
0
0.25
0.50
0.75
1.00
Progression-free survival
0
0.25
0.50
0.75
1.00
Progression-free survival
0
0.25
0.50
0.75
1.00
Progression-free survival
Number at risk
HR: 2.07 (1.21–3.54) P = 0.0067
0 20 40 60 80 Time (months) Number at risk
HR: 7.38 (2.15–25.38) P = 0.0002
0 10 20 30 40 50 60 Time (months) Number at risk
HR: 3.90 (1.05–14.53) P = 0.0286
0 10 20 30 Time (months)
59 22 6 2 0 59 10 3 1 0
42 17 4 2 0 32 7 3 1 0
12 8 5 4 2 2 0 19 3 2 0 0 0 0
6311 10 0 0 0
Number at risk
Low risk High risk
0.5
0.6
0.7
0.8
PLIP
QUILT-1M
BiomedCLIP
CONCH
MUSK
AUC
0.5
0.6
0.7
PLIP
QUILT-1M
BiomedCLIP
CONCH
MUSK
C-index
PFS
P = 0.008
P = 0.02
P = 0.008
P = 0.02
Objective response P = 0.008 P = 0.02 P = 0.02 P = 0.02
0.5
0.6
0.7
0.8
Image
Report
Image+
report
AUC
0.5
0.6
0.7
Image
Report
Image+
report
C-index
PFS
P = 0.008 P = 0.008
Objective response
P = 0.02 P = 0.008
Fig. 5 | Lung cancer immunotherapy response prediction. a, MUSK
substantially outperformed other foundation models in predicting objective response and PFS in patients with NSCLC treated with immunotherapy. b, The multimodal MUSK model significantly improved upon models based on clinical reports or WSIs alone for predicting immunotherapy response and outcomes. c, Kaplan–Meier analysis demonstrated that MUSK significantly stratified patients into high-risk and low-risk groups for PFS in the entire cohort and in clinically relevant subgroups defined by PD-L1 expression and epidermal growth factor receptor (EGFR) mutation status. The two-sided log-rank test was used to compare the survival differences between the high-risk and low-risk groups. d, Two examples of lung cancer cases with and without an
objective response to immunotherapy. In each panel, the left image shows the original WSI, whereas the middle image displays the corresponding heat map that highlights the regions the model focused on within the WSIs. The right images provide enlarged views of the regions receiving the most attention from the model. The case with a response showed abundant infiltration of lymphocytes and minimal stroma. On the other hand, the case without a response showed minimal lymphocyte infiltration and abundant stroma. TPS, tumour proportion score. In a and b, the error bars represent the mean with standard deviation computed from five-fold cross-validation experiments, and the two-sided Mann–Whitney U-test was used to measure the statistical significance between MUSK and the compared methods.


8 | Nature | www.nature.com
Article
We compared the performance of the MUSK model with text-only and image-only models trained based on clinical reports and WSIs alone. MUSK significantly improved upon single-modal methods, demonstrating the effectiveness of our multimodal approach for predicting immunotherapy response and outcomes (Fig. 5b). To assess the ability of MUSK to stratify patients for PFS, we performed Kaplan–Meier analysis (Fig. 5c). In the entire cohort, MUSK separated patients into two risk groups with HR of 2.54 (1.66–3.90); P < 0.0001. The median PFS were 4.3 and 13.4 months for the high- and low-risk groups, respectively. In comparison, tumour PD-L1 expression did not significantly stratify patients for PFS (Supplementary Fig.10a). Our analysis revealed that MUSK can further stratify patients for PFS regardless of PD-L1 expression, EGFR mutation status and treatment regimens with single-agent ICI or chemo–ICI combination therapy (Fig. 5c and Extended Data Fig. 6a). The results are particularly striking for patients with PD-L1-negative (TPS = 0) tumours, with HR of 7.38 (2.15–25.38); P = 0.0002. These findings are clinically significant because patients with PD-L1-negative and EGFR-mutated tumours typically do not receive immunotherapy because of low response rates, but MUSK can identify a subset of these patients who may benefit from immunotherapy. We further performed multivariate Cox regression analyses to evaluate the independent value of MUSK in predicting PFS. We incorporated all relevant clinical variables into the analysis, including age, sex, histology, central nervous system metastases, smoking, EGFR mutation and tumour PD-L1 expression. Our results indicate that MUSK is the most significant predictor of PFS with P = 0.0012 (Supplementary Fig. 9). Overall, these findings demonstrate that by integrating multimodal data, MUSK can provide valuable additional information regarding a patient’s likelihood of response to immunotherapy and, therefore, may potentially inform treatment decision-making. To facilitate the interpretation of the model prediction, we generated attention heat maps and overlaid them on the WSIs (Fig.5d). For patients predicted to have a high likelihood of response, the high-attention areas showed abundant infiltration of lymphocytes and minimal intratumoural stroma. On the other hand, for those with a low likelihood of response, the high-attention areas showed minimal intratumoural lymphocyte infiltration and abundant stroma with dense collagenous fibres. These findings suggest that the model could uncover pathological features previously implicated in immunotherapy response52. Finally, we evaluated the multimodal MUSK for predicting response and outcome in 101 advanced gastro-oesophageal patients treated with ICI-based immunotherapy. The only established predictive biomarker in gastro-oesophageal cancer is microsatellite instability (MSI). In this cohort, MSI-H status had modest accuracy for predicting immunotherapy response, with an AUC of 0.616 (95% CI: 0.550–0.682; P < 0.0001). In comparison, MUSK achieved a much higher AUC of 0.762 (95% CI: 0.718–0.805), which significantly outperformed other multimodal foundation models, such as PLIP, QUILT-1M, BiomedCLIP and CONCH, with AUC between 0.652 and 0.693 (Extended Data Fig.5a). MUSK was also superior to pathology-based foundation models, including UNI, GigaPath and Virchow, with AUC ranging from 0.644 to 0.651. Similar results were obtained for predicting PFS, with MUSK outperforming the other foundation models (Extended Data Fig. 5a). Consistent with the results for lung cancer, the multimodal MUSK model significantly improved upon text-only and image-only models for predicting immunotherapy response and outcomes in gastro-oesophageal cancer (Extended Data Fig. 5b). We performed Kaplan–Meier analysis to assess MUSK for stratifying patient outcomes (Extended Data Fig. 5c). Whereas PD-L1 expression did not significantly stratify patients for PFS (Supplementary Fig.10b), MUSK separated patients into two risk groups with HR of 3.49 (2.02–6.01); P < 0.0001. The median PFS were 3.6 and 14.1 months for the high- and low-risk groups, respectively. MUSK further stratified patients within biomarker-defined subgroups, such as PD-L1-negative
(combined positive score = 0) and PD-L1-positive (combined positive score ≥ 1) tumours as well as microsatellite stable/MSI-L tumours. In addition, MUSK stratified patients regardless of treatment regimens with either single-agent ICI or chemo–ICI combination therapy (Extended Data Fig.6b). Finally, we performed multivariate Cox regression analyses, and our results showed that MUSK was the only significant predictor of PFS (P = 0.0013), in addition to MSI status (Extended Data Fig.5d). Visualization of attention heat maps revealed differential patterns of lymphocyte infiltration and stroma abundance in responders versus non-responders (Extended Data Fig. 5e).
Discussion
In this study, we present MUSK, a new vision–language foundation model for general-purpose oncology applications. Through extensive benchmark evaluation of 23 downstream tasks, we show that MUSK achieves superior performance over existing foundation models, with minimal or no further training, for applications in cancer detection, diagnosis and grading. Importantly, in contrast to previous studies that relied on the similarity between different data modalities, we leveraged the complementary information between clinical reports and images and demonstrated that the multimodal approach achieved superior outcome prediction over either modality alone. Specifically, MUSK showed strong performance in melanoma relapse prediction, prognosis prediction across 16 cancer types and immunotherapy response prediction in two real-world cohorts of lung and gastro-oesophageal cancers. The performance gain achieved by MUSK is largely attributable to its ability to incorporate unpaired image and text data for pretraining, which is far more common than paired data. Existing studies have used off-the-shelf foundation models with contrastive learning, which requires paired image–text data for pretraining. By contrast, MUSK is a custom-designed foundation model pretrained with unified masked modelling. This allowed us to leverage substantially larger and more diverse unpaired data (50 million images and one billion text tokens), which represent an increase of several orders of magnitude over approximately one million image–text pairs used in previous studies15,16. The scarcity of complete multimodal data represents a major challenge for training reliable AI models4. Our approach provides an effective solution to this problem by using more readily available unimodal data for unified masked learning, followed by using multimodal data for fine-tuning and alignment. This training paradigm can be extended and applied in building multimodal foundation AI models in other domains beyond pathology, such as radiology and dermatology images/reports7, as well as structured data, such as genomics53. Accurate prediction of treatment response and outcomes has significant clinical implications for precision oncology20. There are important conceptual and practical distinctions between cancer detection and diagnosis, the primary focus of existing pathology foundation models10–12,16. Given that pathologists have excellent performance in diagnosing cancer (which is the current gold standard), the impact of AI models in such scenarios would be limited to an assistive role. However, outcome prediction is a much more challenging problem because of the inherent uncertainty associated with forecasting the future. Current approaches based on clinical risk factors, such as cancer stage and tumour grade, have limited accuracy, typically with a c-index of approximately 0.65, leaving room for improvement. By combining routine clinical reports and histopathology images, the multimodal MUSK model significantly improved upon traditional risk factors for prognosis prediction across 16 cancer types, with an average c-index of 0.75 and exceeding 0.8 for several cancers. This model may be used to complement the current staging system and refine risk stratification, paving the way for personalized treatment strategies. For instance, in early-stage cancers, adjuvant therapy can be given to patients at a high risk of relapse after curative-intent surgery, whereas low-risk patients may avoid the toxicity associated with systemic drugs.


Nature | www.nature.com | 9
Immunotherapy, particularly ICIs, has prolonged the survival of many patients with metastatic cancers and is the standard of care for the treatment of most tumour types. However, only a small fraction (10–20%) of patients respond to immunotherapy and experience durable clinical benefits54. Given the financial burden and potential for immune-related toxicities of these treatments55, it is crucial to identify patients who are most likely to respond to and benefit from ICIs. Here, we fine-tuned our pretrained multimodal foundation model for predicting immunotherapy response based on routine clinical reports and histopathology images. The model significantly improved upon existing clinical biomarkers, such as PD-L1 expression and MSI status in lung and gastro-oesophageal cancers, which are among the most common and lethal cancers worldwide56. To aid in model interpretation, we applied visualization techniques based on attention heat maps, which revealed pathological features of the tumour microenvironment that were consistent with known mechanisms of response and resistance to immunotherapy52,57. Importantly, the model identified a subset of patients with PD-L1-negative or EGFR-mutated tumours who could benefit from ICIs. Because these patients typically do not receive ICIs because of their overall low response rates54,58, our findings have significant clinical implications with the potential for broadening the population of patients who may benefit from ICIs. Although the results of immunotherapy response prediction are promising, it is worth noting that they were obtained based on relatively small cohorts of about 220 patients from one academic medical centre. Before the model can be considered for clinical implementation and adoption, several steps are needed to ensure that it is rigorously evaluated for safety, efficacy and clinical utility. First, these findings should be validated and confirmed in future studies with larger, multi-institution cohorts. Second, for high-stakes applications, such as treatment decision-making, regulatory approval is required, including validation in prospective clinical trials of immunotherapy-treated patients from diverse populations. The generation of high-level evidence through rigorous prospective validation can ultimately lead to changes in the clinical guidelines and clinical practice. In conclusion, we propose a new vision–language foundation model by leveraging unpaired image–text data. The model provides an effective approach to the integration of pathology images and clinical reports and can potentially improve diagnosis and precision cancer therapy.
Online content
Any methods, additional references, Nature Portfolio reporting summaries, source data, extended data, supplementary information, acknowledgements, peer review information; details of author contributions and competing interests; and statements of data and code availability are available at https://doi.org/10.1038/s41586-024-08378-w.
1. Sammut, S.-J. et al. Multi-omic machine learning predictor of breast cancer therapy response. Nature 601, 623–629 (2022). 2. Vanguri, R. S. et al. Multimodal integration of radiology, pathology and genomics for prediction of response to PD-(L)1 blockade in patients with non-small cell lung cancer. Nat. Cancer 3, 1151–1164 (2022). 3. Acosta, J. N., Falcone, G. J., Rajpurkar, P. & Topol, E. J. Multimodal biomedical AI. Nat. Med. 28, 1773–1784 (2022). 4. Boehm, K. M., Khosravi, P., Vanguri, R., Gao, J. & Shah, S. P. Harnessing multimodal data integration to advance precision oncology. Nat. Rev. Cancer 22, 114–126 (2022). 5. Lipkova, J. et al. Artificial intelligence for multimodal data integration in oncology. Cancer Cell 40, 1095–1110 (2022). 6. Moor, M. et al. Foundation models for generalist medical artificial intelligence. Nature 616, 259–265 (2023). 7. Kim, C. et al. Transparent medical image AI via an image–text foundation model grounded in medical literature. Nat. Med. 30, 1154–1165 (2024). 8. Singhal, K. et al. Large language models encode clinical knowledge. Nature 620, 172–180 (2023). 9. Zhou, Y. et al. A foundation model for generalizable disease detection from retinal images. Nature 622, 156–163 (2023). 10. Xu, H. et al. A whole-slide foundation model for digital pathology from real-world data. Nature 630, 181–188 (2024).
11. Chen, R. J. et al. Towards a general-purpose foundation model for computational pathology. Nat. Med. 30, 850–862 (2024). 12. Vorontsov, E. et al. A foundation model for clinical-grade computational pathology and rare cancers detection. Nat. Med. 30, 2924–2935 (2024). 13. Wang, X. et al. A pathology foundation model for cancer diagnosis and prognosis prediction. Nature 634, 970–978 (2024). 14. Christensen, M., Vukadinovic, M., Yuan, N. & Ouyang, D. Vision–language foundation model for echocardiogram interpretation. Nat. Med. 30, 1481–1488 (2024). 15. Huang, Z., Bianchi, F., Yuksekgonul, M., Montine, T. J. & Zou, J. A visual–language foundation model for pathology image analysis using medical Twitter. Nat. Med. 29, 2307–2316 (2023). 16. Lu, M. Y. et al. A visual-language foundation model for computational pathology. Nat. Med. 30, 863–874 (2024). 17. Lu, M. Y. et al. A multimodal generative AI copilot for human pathology. Nature 634, 466–473 (2024). 18. Radford, A. et al. Learning transferable visual models from natural language supervision. In Proc. Int. Conf. Machine Learning (eds Meila, M. & Zhang, T.) 8748–8763 (PMLR, 2021). 19. Schuhmann, C. et al. LAION-5B: an open large-scale dataset for training next generation image-text models. Adv. Neural Inf. Process. Syst. 35, 25278–25294 (2022). 20. Bhinder, B., Gilvary, C., Madhukar, N. S. & Elemento, O. Artificial intelligence in cancer research and precision medicine. Cancer Discovery 11, 900–915 (2021). 21. Wang, W. et al. Image as a foreign language: BEiT pretraining for vision and visionlanguage tasks. In Proc. IEEE/CVF Conf. Computer Vision Pattern Recognition (eds Brown, M. S., Li, F.-F., Mori, G. & Sato, Y.) 19175–19186 (IEEE, 2023). 22. Gamper, J. & Rajpoot, N. Multiple instance captioning: learning representations from histopathology textbooks and articles. In Proc. IEEE/CVF Conf. Computer Vision Pattern Recognition (eds Brown, M. S., Sukthankar, R., Tan, T. & Zelnik, L.) 16549–16559 (IEEE, 2021). 23. Sun, Y. et a. PathMMU: a massive multimodal expert-level benchmark for understanding and reasoning in pathology. In Eur. Conf. Computer Vision (eds Leonardis, A., Ricci, E., Roth, S., Russakovsky, O., Sattler, T. & Varol, G.) 56–73 (Springer, 2025). 24. Kim, J.-H., Jun, J. & Zhang, B.-T. Bilinear attention networks. In Adv. Neural Inf. Process. Syst. (eds Bengio, S.,Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N. &Garnett, R.). 1571–1581 (PMLR, 2018). 25. Nguyen, B. D. et al. Overcoming data limitation in medical visual question answering. In Proc. Medical Image Computing Computer Assisted Intervention–MICCAI 2019: 22nd Int. Conf. (eds Shen, D. et al.) 522–530 (Springer, 2019). 26. Li, L. H., Yatskar, M., Yin, D., Hsieh, C.-J. & Chang, K.-W. VisualBERT: a simple and performant baseline for vision and language. Preprint at https://arxiv.org/abs/1908.03557 (2019). 27. Naseem, U., Khushi, M., Dunn, A. G. & Kim, J. K-PathVQA: knowledge-aware multimodal representation for pathology visual question answering. IEEE J. Biomed. Health Inf. 28, 1886–1895 (2024). 28. He, X., Zhang, Y., Mou, L., Xing, E. & Xie, P. PathVQA: 30000+ questions for medical visual question answering. Preprint at https://arxiv.org/abs/2003.10286 (2020). 29. Barbano, C. A. et al. Unitopatho, a labeled histopathological dataset for colorectal polyps classification and adenoma dysplasia grading. In 2021 IEEE Int. Conf. Image Processing (ICIP) (eds alZahir, S., Labeau, F. & Mock, K.) 76–80 (IEEE, 2021). 30. Brancati, N. et al. BRACS: a dataset for breast carcinoma subtyping in H&E histology images. Database 2022, baac093 (2022). 31. Veeling, B. S., Linmans, J., Winkens, J., Cohen, T. & Welling, M. Rotation equivariant CNNs for digital pathology. In Proc. Medical Image Computing Computer Assisted Intervention, MICCAI 2018: 21st Int. Conf. (eds Frangi, A., Schnabel, J., Davatzikos, C., Alberola-López, C. & Fichtinger, G) 210–218 (Springer, 2018). 32. Kriegsmann, K. et al. Deep learning for the detection of anatomical tissue structures and neoplasms of the skin on scanned histopathological tissue sections. Front. Oncol. 12, 1022967 (2022). 33. Kumar, N. et al. A multi-organ nucleus segmentation challenge. IEEE Trans. Med. Imaging 39, 1380–1391 (2019). 34. Silva-Rodríguez, J., Colomer, A., Sales, M. A., Molina, R. & Naranjo, V. Going deeper through the gleason scoring scale: an automatic end-to-end system for histology prostate grading and cribriform pattern detection. Comput. Methods Programs Biomed. 195, 105637 (2020). 35. Borkowski, A. A. et al. Lung and colon cancer histopathological image dataset (lc25000). Preprint at https://arxiv.org/abs/1912.12142 (2019). 36. Brummer, O., Pölönen, P., Mustjoki, S. & Brück, O. Integrative analysis of histological textures and lymphocyte infiltration in renal cell carcinoma using deep learning. Preprint at bioRxiv https://doi.org/10.1101/2022.08.15.503955 (2022). 37. Kather, J. N. et al. Predicting survival from colorectal cancer histology slides using deep learning: a retrospective multicenter study. PLoS Med. 16, e1002730 (2019). 38. Arunachalam, H. B. et al. Viable and necrotic tumor assessment from whole slide images of osteosarcoma using machine-learning and deep-learning models. PLoS One 14, e0210706 (2019). 39. Han, C. et al. Multi-layer pseudo-supervision for histopathology tissue semantic segmentation using patch-level classification labels. Med. Image Anal. 80, 102487 (2022). 40. Kather, J. N. et al. Pan-cancer image-based detection of clinically actionable genetic alterations. Nat. Cancer 1, 789–799 (2020). 41. Xu, F. et al. Predicting axillary lymph node metastasis in early breast cancer using deep learning on primary tumor biopsy slides. Front. Oncol. 11, 759007 (2021). 42. Roetzer-Pejrimovsky, T. et al. The digital brain tumour atlas, an open histopathology resource. Sci. Data 9, 55 (2022). 43. Atkins, M. B. et al. The state of melanoma: emergent challenges and opportunities. Clin. Cancer Res. 27, 2678–2697 (2021). 44. Thompson, A. K., Kelley, B. F., Prokop, L. J., Murad, M. H. & Baum, C. L. Risk factors for cutaneous squamous cell carcinoma recurrence, metastasis, and disease-specific death: a systematic review and metaanalysis. JAMA Dermatol. 152, 419–428 (2016).
45. VisioMel. Visiomel Challenge: Predicting Melanoma Relapse (2023) (accessed 1 April 2023); https://www.drivendata.org/competitions/148/visiomel-melanoma/page/674/.


10 | Nature | www.nature.com
Article
46. Ikezogwo, W. et al. Quilt-1m: one million image-text pairs for histopathology. Adv. Neural Inf. Process. Syst. 36, 37995–38017 (2024). 47. Zhang, S. et al. Large-scale domain-specific pretraining for biomedical vision-language processing. Preprint at https://arxiv.org/abs/2303.00915 (2023). 48. Hellmann, M. D. et al. Nivolumab plus ipilimumab in advanced non-small-cell lung cancer. N. Engl. J. Med. 381, 2020–2031 (2019). 49. Gandhi, L. et al. Pembrolizumab plus chemotherapy in metastatic non-small-cell lung cancer. N. Engl. J. Med. 378, 2078–2092 (2018). 50. Samstein, R. M. et al. Tumor mutational load predicts survival after immunotherapy across multiple cancer types. Nat. Genet. 51, 202–206 (2019). 51. Cristescu, R. et al. Pan-tumor genomic biomarkers for PD-1 checkpoint blockade-based immunotherapy. Science 362, eaar3593 (2018). 52. Bagaev, A. et al. Conserved pan-cancer microenvironment subtypes predict response to immunotherapy. Cancer Cell 39, 845–865 (2021). 53. Cui, H. et al. scGPT: toward building a foundation model for single-cell multi-omics using generative AI. Nat. Methods 21, 1470–1480 (2024). 54. Mok, T. S. et al. Pembrolizumab versus chemotherapy for previously untreated, PD-L1expressing, locally advanced or metastatic non-small-cell lung cancer (KEYNOTE-042): a randomised, open-label, controlled, phase 3 trial. Lancet 393, 1819–1830 (2019).
55. Johnson, D. B., Nebhan, C. A., Moslehi, J. J. & Balko, J. M. Immune-checkpoint inhibitors: long-term implications of toxicity. Nat. Rev. Clin. Oncol. 19, 254–267 (2022). 56. Bray, F. et al. Global cancer statistics 2022: Globocan estimates of incidence and mortality worldwide for 36 cancers in 185 countries. CA: A Cancer Journal for Clinicians 74, 229–263 (2024). 57. Bruni, D., Angell, H. K. & Galon, J. The immune contexture and immunoscore in cancer prognosis and therapeutic efficacy. Nat. Rev. Cancer 20, 662–680 (2020). 58. Herbst, R. S. et al. Atezolizumab for first-line treatment of PD-L1-selected patients with NSCLC. N. Engl. J. Med. 383, 1328–1339 (2020).
Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
Springer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law.
© The Author(s), under exclusive licence to Springer Nature Limited 2025


Methods
Model design and pretraining
The pretraining of MUSK, inspired by BEiT3 (ref. 21), consisted of two main steps. The first step used masked data modelling to leverage large-scale unpaired images and text. The second step utilized around one million image–text pairs with contrastive learning to align the two modalities and establish connections between images and texts. The network backbone is a general-purpose multimodal transformer inspired by mixture-of-experts networks in large language models59, multimodal pretraining21,60 and image generation61. The model configurations are specified in Extended Data Fig. 7 and Supplementary Table 16.
Multimodal data curation for pretraining
For pretraining of the multimodal MUSK foundation model, we incorporated unpaired pathology images and texts for masked learning, as well as paired image–text data for contrastive learning. The masked pretraining dataset comprised one billion text tokens extracted from 1,001,800 pathology-related articles from the PubMed Central Open Access Dataset and 50 million pathology image patches from TCGA. The image patches were obtained from nearly 33,000 digitized H&E-stained WSIs from 11,577 patients representing 33 tumour types. We used the QUILT-1M46 dataset (802,000 image–text pairs) in addition to the PathAsst62 dataset (207,000 image–text pairs) for the second pretraining phase via contrastive learning. Noisy image–text pairs collected from the web present challenges for training and may degrade the model performance. Thus, instead of training directly on these datasets, we adopted a bootstrap approach similar to BLIP63 during contrastive learning. We initially trained on QUILT-1M to obtain a baseline model and then filtered out low-similarity image–text pairs based on this model. The final model was trained on the refined image–text dataset with an improved data quality (Supplementary Fig. 2).
Unified masked pretraining
We used a unified masked data modelling approach for pretraining. We sampled a batch of training images and texts during training to apply masked loss and optimize the model. We utilized masked language modelling (MLM) loss for texts and masked image modelling (MIM) loss for images.
Masked language modelling. Similar to bidirectional encoder representations from transformers (BERT)64, we randomly selected 15% of the tokens within a text sequence and replaced them with special [MASK] tokens. The model was then trained to predict these masked tokens using the context provided by all other unmasked text tokens. The positions of the masked tokens are denoted as π ∈ {1, 2, . . . , 0.15K}, where K is the total number of input tokens. The input sequence with the masked tokens is yπ. The output vectors yi,i∈π, corresponding to
the masked token positions, are fed into a classifier. The classifier predicts the most probable words from the vocabulary for each masked position using cross-entropy loss as the objective function:
∑
=− 1 K p
0.15 log ( | ) (1)
ii
MLM
∈ MLM
L yy
π
π
Masked image modelling. The input image ∈ RH×W×C
x was split into N image patches {xi }
p i
N
=1 and then tokenized into z = [z , ..., zN] ∈ V
hw 1
×
as the output labels of MIM using an image tokenizer. The vocabulary V = {1, ..., V }contained discrete token indices. At the input layer, 40% image patches were randomly masked, and then the model predicted the visual tokens zi of the masked patches. The masked positions are denoted as M ∈ {1, ..., 0.4N}. Next, we replaced the masked patches
with a learnable embedding e ∈ D
[M] R , making the input corrupted image patches M M
xM = {x : i ∉ } {e : i ∈ }
i
p i
N i
N =1 [M] =1
⋃ that are fed into the transformer. The pretraining objective was to maximize the log-likelihood of the correct visual tokens zi given the corrupted image:
x
∑
=− 1N p z
0.4 log ( ). (2)
i
MIM i
∈ MIM
L
M
M
An image tokenizer is required to obtain semantically meaningful visual tokens. However, existing tokenizers, such as DALL-E65 and BEiT-v2 (ref. 66), are primarily trained on natural images. Because the image tokenizer defines the learning targets for MIM, using a non-pathology-specific tokenizer could result in suboptimal image representations. To address this, we trained a pathology-specific image tokenizer for MUSK following the BEiT-v2 methodology66, utilizing five million pathology images from the TCGA dataset. For training, we adopted CTransPath67 as the teacher model, providing semantic-aware targets to enhance the tokenizer performance.
Masked training settings. Image augmentations included random vertical flip (P = 0.5), colour dropping (P = 0.2) to convert images to grey scale and weak colour jittering (P = 0.8) with specific adjustments to brightness, contrast, saturation and hue. Additionally, RandStainNA68 and multiple fields of view (FoVs)69, which involve random magnifications at ×10, ×20 and ×40, were incorporated into the training pipeline. We pretrained MUSK for one million steps using the masked pretraining loss of LMIM for images and LMLM for texts. The batch sizes were 2,048 for images and 2,048 for texts. MUSK used an input image with 384 × 384 pixels and then patched as 16 × 16 pixels. Text data were tokenized using the SentencePiece tokenizer with a vocabulary size of 64,000. We used the AdamW70 optimizer with β1 = 0.9, β2 = 0.95 and ε = 1 × 10−8 for optimization. We used a cosine learning rate decay scheduler with a peak learning rate of 1.5 × 10−3 and a linear warmup of 10,000 steps. The weight decay was set as 0.05, and the stochastic depth with a rate of 0.1 was used.
Contrastive pretraining
The second pretraining step utilized contrastive learning to further train MUSK on image–text pairs for modality alignment. Image embeddings and text embeddings were used to compute the contrastive loss
con
L 18. Contrastive loss aims to align the global representation of images and texts. We further designed an auxiliary loss for fine-grained modality alignment. Specifically, we constructed a lightweight cross-attention decoder module utilizing images as side information for MLM. Image embeddings were used as the key and value in cross-attention, whereas language embeddings served as queries. This approach encourages language embedding to explore more detailed interactions with images, ultimately enhancing modality alignment. We empirically masked 30% of the input text tokens and predicted ground-truth labels. We built a prediction layer at the output hidden states of the cross-modal decoder and finally optimized the model through cross-entropy loss
aux
L . The training loss for modality alignment is a combination of contrastive loss and auxiliary MLM loss with the decoder (Extended Data Fig. 7b):
= + 0.1 × (3)
align con aux
LL L
Contrastive training settings. We pretrained MUSK with contrastive learning using the loss function Lalign for 20 epochs. The batch size was 3,072 image–text samples. MUSK used an input image with 384 × 384 pixels and then patched as 16 × 16 pixels. We applied image augmentations, such as random resized cropping, horizontal flipping and colour jittering, to enhance the training data. Text data were tokenized using a SentencePiece tokenizer with a vocabulary size of


Article
64,000. We used the AdamW optimizer with β1 = 0.9, β2 = 0.95 and ε = 1 × 10−8 for optimization. We used a cosine learning rate decay scheduler with a peak learning rate of 1 × 10−4 and a linear warm-up of two epochs. The weight decay was set to 0.05, and the stochastic depth rate was 0.1.
Ablation study
MUSK enhances traditional masked pretraining and contrastive learning by introducing four key adaptations: pathology-specific augmentations (stain augmentations and multiple fields of view), a pathology-specific tokenizer for MIM, a fine-grained imagetext decoder for better local alignment and bootstrapped contrastive learning to filter noisy data. We performed a series of ablation studies that demonstrated that these adaptations are essential for optimizing model performance and significantly improving image representation, cross-modal learning and data quality for precision oncology applications (Extended Data Fig. 8 and Supplementary Results).
Benchmark datasets
We evaluated the MUSK model for multimodal retrieval, VQA and histopathology image classification using various publicly available benchmark datasets. BookSet22 contains 4,265 image–text pairs for cross-modal retrieval, whereas PathMMU23 includes 7,774 annotated image–caption pairs, emphasizing retrieval with expert-reviewed cases. PathVQA28 provides 32,799 open-ended questions linked to 4,998 pathology images for VQA. For histopathology classification, PatchCamelyon31 contains 327,680 binary-labelled images, and NCT-CRC-HE-100K37 spans 107,180 images across nine colorectal tissue classes. SICAPv234 focuses on prostate histology with 12,081 patches in four classes, whereas Osteo38 contains 1,144 osteosarcoma images across three classes. RenalCell36 features 35,458 images for renal carcinoma classification with six tissue types, and SkinCancer32 includes 129,364 patches representing 16 skin conditions. LC2500035 contains 25,000 images split into lung and colon cancer subsets, with five classes in total. PanNuke33 (https://www.drivendata.org/competitions/148/visiomel-melanoma/page/673/) includes over 200,000 nuclei annotations across 19 tissue types for binary classification. UniToPatho29 supports colorectal polyp grading with 9,536 patches in six classes, and WSSS4LUAD39 targets LUAD tissue classification with 10,091 images in three categories. BRACS30 offers 4,539 breast image patches with six lesion types, whereas BCNB41 and MUV-IDH42 include 1,057 and 872 slides, respectively, with multimodal clinical information for breast cancer and brain tumour classification. Detailed descriptions of each dataset are provided in Supplementary Methods.
Melanoma relapse prediction
We evaluated the MUSK model for predicting risk of relapse in melanoma by combining information from histopathology images and clinical reports. For this purpose, we used the VisioMel Challenge dataset45, curated from the French national database on melanoma. This dataset contained clinical reports, diagnostic H&E WSIs and follow-up data for 1,342 patients with melanoma. For comparison, we trained three models based on MUSK: (1) an image-based model, (2) a text-based model and (3) a multimodal classifier that integrates image and text data. To evaluate these models, we conducted five-fold cross-validation through stratified sampling based on relapse. Because MUSK-V (the vision part of MUSK) is a patch-level encoder, we performed patch aggregation using attention-based multiple instance learning (AbMIL)71. It consists of a fully connected layer and rectified linear unit nonlinearity that maps the inputs to an embedding dimension of 512. This stage followed a two-layer gated attention network with a hidden dimension of 384. The network uses a fully connected classifier head that projects attention-pooled slide-level
image embeddings onto the desired dimension. Meanwhile, clinical reports—providing details such as the patient’s age at initial diagnosis, sex, primary site and medical history—were encoded using MUSK-L, the language component of MUSK. Finally, we combined the slide-level image embeddings and text embeddings as input to a lightweight multilayer perceptron classifier, which generated the prediction outputs. We trained each model for 100 epochs on the training set using an AdamW optimizer and a cosine learning rate scheduler with an initial learning rate of 0.001. We used a weighted data sampler that balanced the sampling probability of each outcome label during each epoch. We set up early stopping criteria if the evaluation metric did not improve for ten consecutive epochs. We used dropout at 0.25 after intermediate layers in the network for regularization.
Pan-cancer prognosis prediction
We evaluated the MUSK model for predicting survival outcomes across various cancer types by combining histopathology images and clinical reports. For this purpose, we used 7,927 whole-slide diagnostic H&E images from 6,602 patients across 16 cancers (BLCA, BRCA, CECS, COADREAD, ESCA, GBM, HNSC, LGG, LIHC, LUAD, LUSC, PAAD, RCC, SKCM, STAD and UCEC) in TCGA72. These datasets were selected based on the size of the cohort and ratio of uncensored to censored patients. We excluded cancer types with fewer than 100 patients or fewer than 5% of the patients who had an outcome event. Each diagnostic H&E slide was associated with a detailed pathology report73. The clinical outcome endpoint was disease-specific survival. We trained a prognostic model separately for each cancer type and evaluated its performance using five-fold cross-validation by stratified sampling based on survival status. We compared the multimodal MUSK model with the unimodal approach, which only utilizes histopathology images or clinical reports as input. To obtain a slide-level prediction, we used AbMIL to aggregate the image features. Preprocessing is necessary for clinical reports because their text length exceeds the 100-token limit for MUSK-L. Here, we leveraged a large language model (GPT-4 (ref. 74)) to generate structured reports with more succinct and informative summaries based on full clinical reports. This was achieved by applying expert-designed prompts, as shown in Supplementary Table8. This procedure allowed us to capture the most relevant information pertinent to survival outcomes, such as patient characteristics, tumour size, differentiation, invasion and lymph node metastases. The training details were the same as those for melanoma relapse prediction.
Immunotherapy response prediction
We evaluated the MUSK model for predicting immunotherapy response and outcomes in two real-world NSCLC and gastro-oesophageal cancer cohorts. Both cohorts were obtained from the Stanford University Medical Center, with approval from the institutional review board. The requirement for informed consent was waived for this retrospective analysis. The inclusion criteria were advanced (metastatic or recurrent) NSCLC or gastro-oesophageal cancer (originating from the oesophagus, gastro-oesophageal junction or stomach) treated with anti-PD1 or anti-PD-L1 immune checkpoint blockade between 2018 and 2023, with an available H&E-stained tissue section from a pretreatment core needle or surgical tumour biopsy. Patients were identified using the Stanford Research Repository75. In both cohorts, patients were treated with various lines of immunotherapy with or without concurrent chemotherapy. The best overall response was evaluated through manual curation of radiology reports, and the patients were divided into two groups: responders (complete or partial response) and non-responders (stable or progressive disease). PFS was determined from the date of treatment initiation until the date of progression or death. Patients who did not progress were censored at the date of the last follow-up.


Our study included 118 patients with NSCLC and 101 patients with gastro-oesophageal cancer treated with immunotherapy (Supplementary Tables 11 and 12). We trained multimodal MUSK models using the diagnostic H&E WSIs and associated pathology report to predict the objective response and PFS. We used the AbMIL method to aggregate histopathology image features and obtain a slide-level prediction. We used GPT-4 to standardize clinical reports by applying expert-designed prompts, as shown in Supplementary Table 9 for lung cancer and Supplementary Table 10 for gastro-oesophageal cancer. We concatenated image and report embeddings, which constituted a multimodal embedding for predicting immunotherapy response and outcomes. The models were evaluated through five-fold cross-validation with stratified sampling of relevant outcomes for each cohort. The training details were the same as those for melanoma relapse prediction.
Model visualization
To enhance the model interpretability, we generated heat maps76 to display the regions related to the model predictions on the WSIs. We cropped WSIs into tiles with 85% overlap and calculated the attention scores for each tile to provide more detailed heat maps. These attention scores were normalized to a 0–1 scale, with warmer colours on the heat map indicating higher attention scores, thus highlighting areas more relevant to the predictions of the model. The heat maps were superimposed onto the original WSIs with a semi-transparent overlay.
Statistical analysis
For zero-shot or fine-tuned tasks with independent test sets, we evaluated performance variations using a non-parametric bootstrapping method, deriving 95% CIs from 1,000 bootstrap samples. For five-fold cross-validation tasks, 95% CIs were estimated based on the results from the five folds. The two-sided Mann–Whitney U-test or two-sided Wilcoxon signed-rank test (as indicated in the figure captions) was used to assess statistical significance. We used AUC to evaluate the performance of melanoma relapse prediction and immunotherapy response prediction. We used the c-index to evaluate the performance of the prognostic models for survival endpoints. KaplanMeier curves were generated to assess patient stratification, using the median predicted risk score as the cut-off value. The statistical significance of low-risk and high-risk patient groups was assessed using the log-rank test.
Inclusion and Ethics Statement
The study was approved by the Institutional Review Board of Stanford University (protocol #67432). The requirement for informed consent was waived for this retrospective analysis.
Reporting summary
Further information on the research design is available in the Nature Portfolio Reporting Summary linked to this article.
Data availability
The histopathology and clinical data for TCGA used in this study are publicly available through the following resources: National Cancer Institute Genomic Data Commons Portal (https://portal.gdc.cancer. gov/), cBioPortal (https://www.cbioportal.org/) and TCGA Pathology Reports (https://github.com/tatonetti-lab/tcga-path-reports). Additional datasets used include QUILT-1M (https://github.com/wisdomikezogwo/quilt1m), PathAsst (https://huggingface.co/datasets/ jamessyx/PathCap), PathVQA (https://huggingface.co/datasets/ flaviagiammarino/path-vqa), BookSet and PubmedSet (https://warwick.ac.uk/fac/cross_fac/tia/data/arch), PatchCamelyon (https:// patchcamelyon.grand-challenge.org/), NCT-CRC-HE-100K (https://
zenodo.org/record/1214456), SICAPv2 (https://data.mendeley.com/ datasets/9xxm58dvs3/1), Osteo (https://www.cancerimagingarchive. net/collection/osteosarcoma-tumor-assessment/), RenalCell (https://zenodo.org/records/6528599), SkinCancer (https://www. isic-archive.com/), LC25000 (https://github.com/tampapath/lung_ colon_image_set), PanNuke (https://warwick.ac.uk/fac/cross_fac/tia/ data/pannuke), UniToPatho (https://ieee-dataport.org/open-access/ unitopatho), WSSS4LUAD (https://wsss4luad.grand-challenge.org/ WSSS4LUAD/), BRACS (https://www.bracs.icar.cnr.it/), Visiomel (https://www.drivendata.org/competitions/148/visiomel-melanoma/), PathMMU (https://huggingface.co/datasets/jamessyx/PathMMU), BCNB (https://bcnb.grand-challenge.org/) and MUV-IDH (https://doi. org/10.25493/WQ48-ZGX). The data for patients in the immunotherapy cohorts are subject to controlled access because they contain sensitive information about patient privacy. Requests for access may be submitted to the corresponding author (rli2@stanford.edu) with a brief research plan and require consent to data use agreement. We aim to respond to all data access requests within 4 weeks. Data usage is restricted to non-commercial academic research purposes.
Code availability
Our work is publicly available at https://github.com/lilab-stanford/ MUSK, including installationinstructions, model weights, evaluation code, and example data.
59. Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q. V., Hinton, G. E., & Dean, J. Outrageously large neural networks: the Sparsely-Gated Mixture-of-Experts layer. Int. Conf. Learning Representations (eds Bengio, Y. & LeCun, Y.) 1–19 (OpenReview.net, 2017). 60. Bao, H. et al. Vlmo: unified vision-language pre-training with mixture-of-modality-experts. Adv. Neural Inf. Process. Syst. 35, 32897–32912 (2022).
61. Esser, P. et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first Int. Conf. Machine Learning (eds Salakhutdinov, R., Kolter, Z., Heller, K., Weller, A., Oliver, N., Scarlett, J. & Berkenkamp, F.) 12606–12633 (PMLR, 2024). 62. Sun, Y. et al. PathAsst: a generative foundation AI assistant towards artificial general intelligence of pathology. In AAAI Conf. Artificial Intelligence (ed. Wooldridge, M.) 5034–5042 (AAAI, 2024). 63. Li, J., Li, D., Xiong, C. & Hoi, S. C. H. BLIP: bootstrapping language-image pre-training for unified vision-language understanding and generation. In Int. Conf. Machine Learning (eds Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G. & Sabato, S.) 12888–12900 (PMLR, 2022). 64. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: pre-training of deep bidirectional transformers for language understanding. In North American Chapter Assoc. Comp. Linguistics (eds Burstein, J., Doran, C., Pedersen, T. & Solorio, T.) 4171–4186 (ACL, 2019). 65. Ramesh, A. et al. Zero-shot text-to-image generation. In Int. Conf. Machine Learning (eds Meila, M. & Zhang, T.) 8821–8831 (PMLR, 2021). 66. Peng, Z., Dong, L., Bao, H., Ye, Q. & Wei, F. BEiT v2: masked image modeling with vectorquantized visual tokenizers. Preprint at https://arxiv.org/abs/2208.06366 (2022). 67. Wang, X. et al. Transformer-based unsupervised contrastive learning for histopathological image classification. Med. Image Anal. 81, 102559 (2022). 68. Shen, Y., Luo, Y., Shen, D. & Ke, J. RandStainNA: learning stain-agnostic features from histology slides by bridging stain augmentation and normalization. In Int. Conf. Medical
Image Computing and Computer-Assisted Intervention (eds Wang, L., Dou, Q., Fletcher, P. T., Speidel, S. & Li, S.) 212–221 (Springer, 2022). 69. Kang, M., Song, H., Park, S., Yoo, D. & Pereira, S. Benchmarking self-supervised learning on diverse pathology datasets. 2023 IEEE/CVF Conf. Computer Vision Pattern Recognition (CVPR) (eds Chellappa, R., Matas, J., Quan, L. & Shah, M.) 3344–3354 (IEEE, 2022). 70. Loshchilov, I. & Hutter, F. Decoupled weight decay regularization. In Int. Conf. Learning Representations (Tara Sainath, T.) 1–18 (OpenReview.net, 2019). 71. Ilse, M., Tomczak, J. & Welling, M. Attention-based deep multiple instance learning. In Int. Conf. Machine Learning (eds Dy, J. & Krause, A.) 2127–2136 (PMLR, 2018). 72. Weinstein, J. N. et al. The cancer genome atlas pan-cancer analysis project. Nat. Genet. 45, 1113–1120 (2013). 73. Kefeli, J. & Tatonetti, N. TCGA-reports: a machine-readable pathology report resource for benchmarking text-based AI models. Patterns 5, 100933 (2024). 74. Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S. & Avila, R. Gpt-4 technical report. arXiv https://arxiv.org/abs/2303.08774 (2023). 75. Callahan, A. et al. The Stanford Medicine data science ecosystem for clinical and translational research. JAMIA Open 6, ooad054 (2023). 76. Lu, M. Y. et al. Data-efficient and weakly supervised computational pathology on wholeslide images. Nat. Biomed. Eng. 5, 555–570 (2021).
Acknowledgements This study was supported in part by the National Institutes of Health under research grant nos. R01CA222512, R01CA233578, R01CA269559, R01CA285456 and R01CA290715 from the National Cancer Institute; grant no. R01DE030894 from the National Institute of Dental and Craniofacial Research; and a research grant from the Stanford Institute


Article
for Human-Centered Artificial Intelligence. K.-H.Y. is supported in part by the National Institute of General Medical Sciences grant no. R35GM142879, the National Heart, Lung, and Blood Institute grant no. R01HL174679, the Department of Defense Peer Reviewed Cancer Research Program Career Development Award no. HT9425-231-0523 and Research Scholar grant no. RSG-24-1253761-01-ESED.
Author contributions J.X., X.W., S.Y. and R.L. conceived and designed the study. R.L., Y.X., F.E., Y.C., C.B., M.G., T.K., S.W., F.M.O., J.N., J.J.N. and M.D. were responsible for private data acquisition. J.X., X.W., Y.X., S.Y. and R.L. acquired the public data. J.X., X.W. and S.Y. performed statistical analyses. J.X., X.W. and S.Y. trained and validated the deep learning network. R.L., J.X., X.W., X.Z., K.-H.Y., Y.C., Y.L., S.Y. and J.N. implemented quality control of data and algorithms. R.L., J.X., X.Z. and Y.C. verified the underlying raw data. J.X., X.W., S.Y., X.Z. and R.L. prepared the first draft of the manuscript. R.L. revised the manuscript. All the
authors contributed to the preparation of the manuscript and approved its submission for publication.
Competing interests A provisional patent related to this work has been filed by Stanford University (US patent application 63/724,237).
Additional information
Supplementary information The online version contains supplementary material available at https://doi.org/10.1038/s41586-024-08378-w. Correspondence and requests for materials should be addressed to Sen Yang or Ruijiang Li. Peer review information Nature thanks Mahmoud Assran, Joe Yeong and the other, anonymous, reviewer(s) for their contribution to the peer review of this work. Reprints and permissions information is available at http://www.nature.com/reprints.


Extended Data Fig. 1 | See next page for caption.


Article
Extended Data Fig. 1 | MUSK for image-to-image retrieval and image
classification. a, We perform zero-shot image retrieval on the UniToPatho dataset, and MUSK outperforms other vision–language foundation models. Data are represented as mean with 95% confidence intervals. Error bars represent the 95% confidence intervals, estimated using the bootstrap method with 1000 replicates. The two-sided Wilcoxon signed-rank test is used to calculate the statistical differences between MUSK and the top-performing compared method (p < 0.0001 in Recall@1, Recall@3, Recall@5, and mMv@5). b, Zero-shot image retrieval on the BRACS dataset. MUSK significantly outperforms other foundation models across various recall levels with p-values of 0.02, 0.07, 0.04, and 0.03 in Recall@1, Recall@3, Recall@5, and mMv@5 metrics, respectively.
Two examples of image retrieval results with the top 3 candidates are shown. DCIS: ductal carcinoma in situ; IBC: invasive breast carcinoma. c, We evaluate the labelling efficiency of various models under a few-shot learning scenario by varying the number of training labels per class. We present results for the [1,2,4,8,10]-shot classification across multiple datasets: LC2500035, UniToPatho29, NCT-CRC37, and BRACS (6 cls)30. The average accuracy shows that MUSK consistently outperforms existing models across these benchmarks. In these box plots, the central lines indicate the median, box bounds are the 25th and 75th percentiles, and whiskers extend to 1.5 times the interquartile range. Each task in the experiments is represented by n = 10 data points generated using 10 independent experiments.


Extended Data Fig. 2 | MUSK for supervised image classification. a, 10-shot classification performance across 12 benchmarks compared with seven alternative vision–language models regarding classification balanced accuracy. The two-sided Wilcoxon signed-rank test is used to assess the statistical differences between MUSK and the compared methods in the 12 benchmark datasets: BRACS (3-cls) (p = 0.43), UniToPatho (p = 0.002), BRACS (6-cls) (p = 0.002), SICAPv2 (p = 0.01), PatchCamelyon (p = 0.006), LC25000 (p = 0.002), PanNuke (p = 0.23), RenalCell (p = 0.002), SkinCancer (p = 0.01), NCT-CRC-HE100K (p = 0.55), Osteo (p = 0.04), and WSSS4LUAD (p = 0.006). b , Linear probe
classification results on 12 benchmark datasets compared with seven alternative models. The two-sided Wilcoxon signed-rank test is used to calculate the statistical differences between MUSK and the compared methods in the 12 benchmark datasets. P-values are observed as follows: BRACS (3-cls) (p = 0.002), UniToPatho(p = 0.002),BRACS(6-cls)(p = 0.01),SICAPv2(p = 0.13),PatchCamelyon (p = 0.002), LC25000 (p = 0.002), PanNuke (p = 0.002), RenalCell (p = 0.002), SkinCancer (p = 0.002), NCT-CRC-HE-100K (p = 0.55), Osteo (p = 0.002), and WSSS4LUAD (p = 0.002). In a and b, we present means with error bars representing the 95% CI, which are computed from n = 10 independent experiments.


Article
Extended Data Fig. 3 | Comparison of MUSK with state-of-the-art pathology foundation models on slide-level benchmark tasks. The comparison methods include both unimodal pathology foundation models (UNI, GigaPath, and Virchow) and multimodal pathology foundation models (PLIP and CONCH). a, Biomarker prediction. AUC results for predicting ER, PR, and HER2 status in the BCNB test set, as well as IDH status in the MUV-IDH dataset. b, Immunotherapy response prediction. Performance in terms of AUC and c-index for lung and
gastro-oesophageal cancers, respectively. c, Prognosis prediction. c-index results for prognosis predictions across 16 TCGA cohorts. MUSK significantly outperforms the compared methods as shown in the overall bars (p-value < 0.0001), representing the average performance across 16 projects. In a-c, data are represented as mean with standard deviations, based on 5-fold crossvalidation experiments. The two-sided Mann-Whitney U test is used to assess the statistical significance between MUSK and the comparison methods.


Extended Data Fig. 4 | See next page for caption.


Article
Extended Data Fig. 4 | Melanoma relapse prediction. a,b, MUSK achieves superior performance for predicting the 5-year risk of relapse in a cohort of 1,342 melanoma patients compared with existing multimodal pathology foundation models. c, At 90% sensitivity for relapse prediction, MUSK substantially improved the specificity by about 15% over other foundation models. d, The multimodal MUSK model significantly improves upon relapse prediction over models based on clinical reports or WSIs alone. e, Two examples of melanoma cases with and without relapse. In each panel, the left image shows the original WSI, while the middle image displays the corresponding heatmaps that highlights the regions model focused on within the WSIs. The right images
provide zoomed-in views of the regions receiving the most model attention. The case with relapse shows the presence of skin ulceration with abundant intratumoral macrophages accompanied by fibrosis, less intratumoral and peritumoral lymphocytes, and brisk mitotic activity. On the other hand, the case without relapse shows an intact epidermis without ulceration, abundant intratumoral and peritumoral lymphocytes, and inconspicuous mitotic activity. In a, c, and d, data are presented as mean values +/− SD from 5-fold cross-validation experiments, and the two-sided Mann-Whitney U test is used to measure the statistical significance between MUSK and the compared methods.


Extended Data Fig. 5 | See next page for caption.


Article
Extended Data Fig. 5 | Gastro-oesophageal cancer immunotherapy response prediction. a, MUSK outperforms other foundation models for predicting objective response and progression-free survival in gastro-esophageal cancer patients treated with immunotherapy. b, The multimodal MUSK model improves upon models based on clinical reports or WSIs alone. c, Kaplan-Meier analysis demonstrates that MUSK significantly stratifies patients into high-risk and lowrisk groups for progression-free survival, in the entire cohort and clinically relevant subgroups. The two-sided log-rank test is used to compare the survival differences between the high-risk and low-risk groups. HR: hazard ratio. d, Multivariate Cox regression analysis shows MUSK is the only significant predictor of progressionfree survival beside MSI status. We computed the P-values using the two-sided Wald test and we presented the HR with 95% confidence intervals. e, Two examples of gastro-oesophageal cancer cases with and without objective response to
immunotherapy. In each panel, the left image shows the original WSI, while the middle image displays the corresponding heatmaps that highlights the regions model focused on within the WSIs. The right images provide zoomed-in views of the regions receiving the most model attention. The case with response shows abundant infiltration of lymphocytes within and around the tumour; the stroma is less fibrotic and displays more oedema. On the other hand, the case without response shows minimal lymphocyte infiltration and increased intra-tumoral and peri-tumoral fibrotic stroma. CPS: combined positive score; MSI/MSS: microsatellite instable/stable; ADC: adenocarcinoma; SCC: squamous cell carcinoma. In a and b, the error bars represent the mean with standard deviation computed from 5-fold cross-validation experiments, and the two-sided MannWhitney U test is used to measure the statistical significance between MUSK and the compared methods.


Extended Data Fig. 6 | Kaplan-Meier analysis of the MUSK model for stratifying patients under different treatment regimens. a,b, The results demonstrate that MUSK significantly stratifies patients for progression-free survival into low- and high-risk groups, in (a) lung and (b) gastro-esophageal
cancers, treated with immunotherapy with or without concurrent chemotherapy. The two-sided log-rank test is used to compare the survival differences between the high-risk and low-risk groups.


Article
Extended Data Fig. 7 | MUSK model configuration. a, MUSK integrates two independent transformers for image and text data modalities. This architecture processes sequences within each modality independently and fuses them in the attention modules, allowing cross-modal interactions and ensuring robustness across different data types. b, During the second phase of
pretraining (Fig. 1), MUSK requires modality alignment using contrastive loss as its training objective, augmented with an auxiliary MLM loss. This MLM component utilizes a streamlined cross-attention decoder that employs text embeddings as queries to interact dynamically with image embeddings, thereby instilling intricate cross-modal insights.


Extended Data Fig. 8 | Ablation study on training configuration. We conducted ablation studies to evaluate the impact of various training configurations (refer to theSupplementary Materials for detailed descriptions). a, The effect of mask pretraining. b, The effect of data distribution, comparing natural images/text with pathology images/text. c, The impact of the data scale for mask pretraining was evaluated using Quilt1M, 15 M images with 500 M text tokens, and 50 M images with 1B text tokens. d, The model capacity of the MUSK models. In a-d, the error bars represent the average of the standard deviations within the datasets. Error bars are not provided for the I2T retrieval, T2I retrieval, and VQA tasks because
they are evaluated on a single dataset. The evaluation metrics used are balanced accuracy for the linear probe classification, 10-shot classification, and zero-shot classification tasks; accuracy for the VQA task; Recall@50 for the I2T and T2I retrievals; and mMV @5 for the I2I retrieval. T2I: Text-to-Image; I2T: Image-to-Text; I2I: Image-to-Image; VQA: Visual Question Answering; cls: classification. The two-sided Mann-Whitney U test is employed to assess the statistical significance of differences between the methods being compared. In a-d, n denotes the number of datasets corresponding to each task.